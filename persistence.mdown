# Persistence
## Hard Disk Drives
In this chapter, we dive into more detail about one device in particular: the **hard disk drive**. These drives have been the main form of persistent data storage in computer systems for decades and much of the development of file system technology is predicated on their behavior.

### The Interface
Let's start by understanding the interface to a modern disk drive. The basic interface for all modern drives is straightforward. The drive consists of a large number of sectors (512-byte blocks), each of which can be read or written. The sectors are numbered from *`0`* to *`n - 1`* on disk with *`n`* sectors. Thus, we can view the disk as an array of sectors.

Multi-sector operations are possible; indeed, many file systems will read or write 4KB at a time (or more). However, when updating the disk, the only guarantee drive manufactures make is that a single 512-byte write is **atomic** (i.e., it will either complete in its entirety or it won't complete at all); thus, if an untimely power loss occurs, only a portion of a larger write may complete.

### Basic Geometry
Let's start to understand some of the components of a modern disk. We start with a **platter**, a circular hard surface on which data is stored persistently by inducing magnetic changes to it. A disk may have one or more platters; each platter has 2 sides, each of which is called a **surface**. These platters are usually made of some hard material (such as aluminum), and then coated with a thin magnetic layer that enables the drive to persistently store bits even when the drive is powered off.

The platters are all bound together around the **spindle**, which is connected to a motor that spins the platters around (while the drive is powered on) at a constant rate.

Data is encoded on each surface in concentric circles of sectors; we call one such concentric circle a **track**. A single surface contains many thousands and thousands of tracks, tightly packed together, with hundreds of tracks fitting into the width of a human hair.

To read and write from the surface, we need a mechanism that allows us to either sense (i.e., read) the magnetic patterns on the disk or to induce a change in (i.e., write) them. This process of reading and writing is accomplished by the **disk head**; there is one such head per surface of the drive. The disk head is attached to a single **disk arm**, which moves across the surface to position the head over the desired track.

### A simple Disk Drive

Let's understand how disk work by building up a model one track at a time. Assume we have a simple disk with a single track.

This track has just 12 sectors, each of which is 512 bytes in size, and addressed therefore by the numbers 0 through 11. The single platter we have here rotates around the spindle, to which a motor is attached.

In the figure, the disk head, attached to the end of the arm, is positioned over sector 6, and the surface is rotating counter-clockwise.

![A Single Track Plus A Head](disk_single_track.png)

**Single-track Latency: The Rotational Delay**

To understand how a request would be processed on our simple, one-track disk, imagine we now receive a request to read block 0.

In our simple disk, the disk doesn't have to do much. In particular, it much just wait for the desired sector to rotate under the disk head. This wait happens often enough in modern drives, and is an important component of I/O service time. that it has a special name: **rotational delay**. A worst case request on this single track would be to sector 5, causing a full rotational delay in order to service such a request.

**Multiple Tracks: Seek Time**

Let's thus look at ever-so-slightly more realistic disk surface, this one with three tracks.

In the figure, the head is currently positioned over the innermost track (which contains sectors 24 through 35); the next track over contains the next set of sectors (12 through 23), and the outermost track contains the first sectors (0 through 11).

To understand how the drive might access a given sector, we now trace what would happen on a request to a distant sector, e.g., a read to sector 11. To service this read, the drive has first move the disk arm to the correct track (in this case, the outermost one), in a process known as a **seek**.

The seek, it should be noted, has many phases: first an *acceleration* phase as the disk arm gets moving; then *coasting* as the arm is moving at full speed, then *deceleration* as the arm slows down; finally *settling* as the head is carefully positioned over the correct track. The **settling time** of often significant, e.g., 0.5 to 2 ms, as the drive must be certain to find the right track.

After the seek, the disk arm has positioned the head over the right track. 

![Three Tracks Plus A Head (Right: With Seek)](disk_multiple_track.png)

As we can see, during the seek, the arm has been moved to the desired track, and the platter of course has rotated, in this case about 3 sectors. Thus, sector 9 is just about to pass under the disk head, and we must only endure a short rotational delay to complete the transfer.

When sector 11 passes under the disk head, the final phase of I/O will take place, known as the **transfer**, where data is either read from or written to the surface. ANd thus, we have a complete picture of I/O time: first a seek, then waiting for the rotational delay, and finally the transfer.

**Some other details**

Finally an important part of nay modern disk drive is its **cache**. This cache is just some small amount of memory which the drive can use to hold data read from or written to the disk. For example, when reading a sector from the disk, the drive might decide to read in all of the sectors on that track and cache them in its memory; doing so allows the drive to quickly respond to any subsequent requests to the same track.

On writes, the drive has a choice: should it acknowledge the write has completed when it has put the data it in its memory, or after the write has actually been written to disk? The former is called **write back** caching, and the latter **write through**. Write back caching sometimes makes the drive appear "faster", but can be dangerous. 

## File System Implementation
### Overall Organization
We now develop the overall on-disk organization of the data structure of the vsfs file system. The first thing we'll need to do is divide the disk into blocks; simple file systems use just one block size, and that's exactly what we'll do here. Let's choose a commonly-used size of 4KB.

Thus, our view of the disk partition where we're building our file system is simple: a series of blocks, each of size 4KB. The blocks are addressed from *`0`* to *`N - 1`*, in a partition of size *`N`* 4-KB blocks. Assume we have a really small disk, with just 64 blocks.

Let's now think about what we need to store in these blocks to build a file system. Of course, the first thing that comes to mind is user data. In fact, most of the space in any file system is (and should be) user data. Let's call the region of the disk we use for user data the **data region**, again for simplicity, reserve a fixed portion of the disk for these blocks, say the last 56 of 64 blocks on the disk.

the file system has to track information about each file. This information is a key piece of **metadata**, and tracks things like which data blocks (in the data region) comprise a file, the size of the file, its owner and access rights, access and modify times, and other similar kinds of information. To store this information, file systems usually have a structure called an **inode**.

To accommodate inodes, we'll need to reserve some space on the disk for them as well. Let's call this portion of the disk the **inode table**, which simply holds an array of on-disk inodes.

We should note here that inodes are typically not that big, for example 128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hold 16 inodes, and our file system above contains 80 total inodes. In our simple file system, built on a tiny 64-block partition, this number represents the maximum number of files we can have in our file system.

Our file system thus far has data blocks (D), and inodes (I), but a few things are still missing. One primary component that is still needed, as you might have guessed, is some way to track whether inodes or data blocks are free or allocated.

We instead choose a simple and popular structure known as a **bitmap**, one for the data region (the **data bitmap**), and one for the inode table (the **inode bitmap**). A bitmap is a simple structure: each bit is used to indicate whether the corresponding object/block is free (0) or in-use (1).

![File System Data Structure](file_system_data_structure.png)

The careful reader may have noticed there is one block left in the design of the on-disk structure of our very simple file system. We reserve this for the **superblock**, denoted by an S in the diagram below. The superblock contains information about this particular file system, including, for example, how many inodes and data blocks are in the file system (80 and 56, respectively in this instance), where the inode table begins (block 3), and so forth. It will likely also include a magic number of some kind to identify the file system type (in this case, vsfs).

## Crash Consistency: FSCK and Journaling
### A detailed example
We need to use a **workload** that updates on-disk structures in some way. Assume here that the workload is simple: the append of a single data block to an existing file. The append is accomplished by opening the file, calling `lseek()` to move the file offset to the end of the file, and then issue a 4KB write to the file before closing it.

This tiny example includes an **inode bitmap** (with just 8bits, one per inode), a **data bitmap** (also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and spread across four blocks), and data blocks (8 total, numbered 0 to 7). Here is a diagram of this file system:
![](crash_consistency_fig1.png)

If you look at the structures in the picture, you can see that a single inode is allocated (inode number 2), which is marked in the inode bitamp, and a single allocated data block (data block 4), also marked in the data bitmap. The inode is denoted I[v1], as it is the first version of this inode; it will soon be updated.

Let's peek insode this simplified inode too. Inside of I[v1], we see:
```
owner       : remzi
permissions : read-write
size:       : 1
pointer     : 4
pointer     : null
pointer     : null
pointer     : null
```
In this simplified example, the `size` of the file is ` (it has one block allocated), the first pointer points to block 4 (the first block of the file, Da), and all three other pointers are set to null (indicating they are not used).

When we append to the file, we are adding a new data block to it, and must update three on-disk structures: the inode (which must point to the new block as well as have a bigger size due to the append), the new data block Db, and a new version of the data bitmap (call it B[v2]) to indicate that new data block has been allocated.

Thus, in memory of the system, we have three blocks which we must write to disk. The updated inode (inode version 2, or I[v2] for short) now looks like this:
```
owner       : remzi
permissions : read-write
size:       : 2
pointer     : 4
pointer     : 5
pointer     : null
pointer     : null
```
The updated data bitmap (B[v2]) now looks like this: 00001100. Finally, there is the data block (Db), which is just filled with whatever it is users put into files.

What we would like is for the final on-disk image of the file system to look like this:
![](crash_consistency_fig2.png)

To achieve this transition, the file system must perform three separate writes to disk, one each for the inode(I[v2]), bitmap (B[v2]), and the data block. Note that these writes usually don't happen immediately when the user issues a `write()` system call; rather, the dirty inode, bitmap, and new data will sit in main memory (int the **page cache** or **buffer cache**) for some time first; then, when the file system finally decides to write them to disk (after say 5 seconds or 30 seconds), the file system will issue the requisite write requests to the disk. Unfortunately, a crash may occur and thus interfere with these updates to the disk. In particular, if a crash happens after one or two of these writes have taken place, but not all three, the file system could be left in a funny state.

To understand the problem better, let's look at some example crash scenarios. Image only a single write succeeds; there are thus three possible outcomes, which we list here:
- **Just the data block (Db) is written to disk.** In this case, the data is on disk, but there is no inode that points to it and no bitmap that even says the block is allocated.
- **Just the updated inode (I[v2]) is written to disk.** In this case, the inode points to to the disk address (5) where Db was about to be written, but Db has not yet been written there. Thus, if we trust that pointer, we will read **garbage** data from the disk (the old contents of disk address 5).

   Furthermore, we have a new problem, which we call a **file-system inconsistency**. The on-disk bitmap is telling us that data on block 5 has not been allocated, but the inode is saying it has. This disagreement in the file system data structures is an inconsistency in the data structures of the file system; to use the file system, we must somehow resolve this problem.
- **Just the updated bitmap (B[v2]) is written to disk.** In this case, the bitmap that block 5 is allocated, but there is no inode that points to it. Thus the file system is inconsistent again; if left unresolved, this write would result in a **space leak**, as block 5 would never be used by the file system.

There are three more crash scenarios in this attemp to write three blocks to disk. In these case, two writes succeed and the last one fails:
- **The inode (I[v2]) and bitmap (B[v2]) are written to disk, but no data (Db).** In this case, the file system metadata is completely consistent: the inode has pointer to block 5, the bitmap indicates that 5 is in use, and thus everything looks OK from the perspective of the file system's metadata. But there is one problem: 5 has garbage in it again.
- **The inode (I[v2]) and the data block (Db) are written, but not the bitmap(B[v2]).** In this case, we have the inode pointing to the correct data on disk, but again have an inconsistency problem between the inode and the old version of the bitmap (B1). Thus, we once again need to solve the problem before using the file system.
- **The bitmap (B[v2]) and data block (Db) are written, but not the inode (I[v2]).** In this case, we again have an inconsistency problem between the inode and the data bitmap. However, even though the block was written and the bitmap indicates its usage, we have no idea which file it belongs to, as no inode points to the file.

### Journaling (or Write-Ahead Logging)
The basic idea is as follows. When updating the disk, before overwriting the structures in place, first write down a little note (somewhere else on the disk, in a well-known location) describing what you are about to do. Writing this note is the "write ahead" part, and we write it to a structure that we organize as a "log"; hence, write-ahead logging.

By writing the notes to disk, you are guaranteeing that if a crash takes place during the update (overwrite) of the structures you are updating, you can go back and look at the note you made and try again; thus, you will know exactly what to fix (and how to fix it) after a crash. By design, journaling adds a bit of work during updates to greatly reduce the amount of work required during recovery.

We'll now describe how **Linux ext3**, a popular journaling file system, incorporates journaling into the file system. Most of the on-disk structures are identical to **Linux ext2**, e.g., the disk is divided into block groups, and each block group has an inode and data bitmap as well as inodes and data blocks. The new key structure is the journaling itself. which occupies some small amount of space within the partition or on another device. Thus, an ext2 file system (without journaling) looks like this:
![](crash_consistency_fig3.png)

Assuming the journal is placed within the same file system image (though sometimes it is placed on a separate device, or as a file within the file system), an ext3 file system with a journal looks like this:
![](crash_consistency_fig4.png)

The real difference is just the presence of the journal, and of course, how it is used.

Data journaling is available as mode with the Linux ext3 file system, from which much of this discussion is based.

Say we have canonical update again, where we wish to write the inode (I[v2]), bitmap (B[v2]) and data block (Db) to disk again. Before writing them to their final disk locations, we are now first going to write them to the log (a.k.a journal). This is what this will look like in the log:
![](crash_consistency_fig5.png)

You can see we have written five blocks here. The transactional begin (TxB) tells us about this update, including information about pending update to the file system (e.g., the final address of the block (I[v2]), B[v2], and Db) as well as some kind of **transactional identifier (TID)**. The middle three blocks just contain the exact contents of the blocks themselves, this is known as **physical logging** as we are putting the exact physical contents of the update in the journal (an alternative idea, **logical logging**, puts a more compact logical representation of the updates in the journal, e.g., "this update wishes to append data block Db to file X", which is a little more complex but can save space in the log and perhaps improve performance). The final block (TxE) is a marker of the end of this transaction, and will also contain the TID.

Once this transaction is safely on disk, we are ready to overwrite the old structure in the file system; this process is called **checkpointing**. Thus, to **checkpoint** the file system (i.e., bring it up to date with the pending update in the journal), we issue the writes I[v2], B[v2], and Db to their disk locations as seen above; if these writes complete successfully, we have successfully checkpointed the file system and are basically done. Thus, our initial sequence of operations:
- **Journal writing:** Write the transaction, including a transaction-begin block, all pending data and metadata updates, and a transaction-end block, to the log; wait for these writes to complete.
- **Checkpoint:** Write the pending metadata and updates to their final locations in the file system.
In our example, we would write TxB, I[v2], B[v2], Db, and TxE to the journal first. When those writes complete, we would complete the updates by checkpointing I[v2], B[v2], and Db, to their final locations on disk.

Things get a little tricker when a crash occurs during the writes to the journal. Here, we are trying to write the set of blocks in the transaction (e.g., TxB, I[v2], B[v2], Db, TxE) to disk. Ideally, we'd like to issue all five block writes at once. However, this is unsafe, for the following reason: given such a big write, the disk internally may perform scheduling and complete small pieces of the big write in any order. Thus, the disk internally may (1) write TxB, I[v2], B[v2], and TxE and only later (2) write Db. Unfortunately, if the disk loses power between (1) and (2), this is what ends up on disk:
![](crash_consistency_fig6.png)

Why is this a problem? Well, the transaction looks like a valid transaction (it has a begin and and end with matching sequence numbers). Furthermore, the file system can't look at that fourth block and know it is wrong; after all, it is arbitrary data. Thus, if the file system now reboots and runs recovery, it will replay this transaction, and ignorantly copy the contents of the garbage block "??" to the location where Db is supposed to live. This is bad for arbitrary user data in a life; it is much worse if it happens to a critical piece of the file system, such as the superblock, which could render the file system unmountable.

To avoid this problem, the file system issues the transaction write in two steps. First, it writes all blocks except the TxE block to the journal, issuing these writes all at once. When these writes are complete, the journal will look something like this:
![](crash_consistency_fig7.png)

When those writes complete, the file system issues the write of the TxE block, thus leaving the journal in this final, safe state:
![](crash_consistency_fig8.png)

An important aspect of this process is the atomicity guarantee provided by the disk. It turns out that the disk guarantees that any 512-byte write will either happen or not (and never be half-written); thus, to make sure the write of TxE is atomic, one should make it a single 512-byte block. Thus, our current protocol to update the file system, with each of its three phases labeled:
- **Journal write:** Write the contents of the transaction (including TxB, metadata, and data) to the log; wait for these writes to complete.
- **Journal commit**: Write the transaction commit block (containing TxE) to the log; wait for write to complete; transaction is said to be **committed**.
- Checkpoint: Write the contents of the update (metadata and data) to their final on-disk locations.

Let’s now understand how a file system can use the contents of the journal to **recover** from a crash. A crash may happen at any time during this sequence of updates. If the crash happens before the transaction is written safely to the log, then our job is easy: the pending update is simply skipped. If the crash happens after the transaction has committed to the log, but before the checkpoint is complete, the file system can **recover** the update as follows. When the system boots, the file system recovery process will scan the log and look for transactions that have committed to the disk; these transactions are thus **replayed** (in order), with the file system again attempting to write out the blocks in the transaction to their final on-disk locations. This form of logging is one of the simplest forms there is, and is called **redo logging**.
