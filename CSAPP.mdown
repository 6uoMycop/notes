<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Computer Systems: A Programmer's Perspective
## Program Structure and Execution
### Representing and Manipulating Information
#### Information Storage
##### Hexadecimal Notation
A single byte consists of 8 bits. In binary notation, its value ranges from \\( 00000000_2 \\) to \\( 11111111_2 \\). Hexadecimal (or simply "hex") uses digits '0' through '9' along with characters 'A' through 'F' to represent 16 possible values. Written in hexadecimal, the value of a single byte can range from \\( 00_{16} \\) to \\( FF_{16} \\).

![Hexadecimal notation.](hex_notation.png)

In C, numeric constants starting with `0x` or `0X` are interpreted as being in hexadecimal. A common task in working with machine-level programs is to manually convert between decimal, binary, and hexadecimal representations of bit patterns. Converting between binary and hexadecimal is straightforward, since it can be performed one hexadecimal digit at a time. Digits can be converted by referring to a chart such that shown above.

##### Data Sizes
The C language supports multiple data formats for both integer and floating-point data.

The exact numbers of bytes for some data types depends on how the program is compiled. We show sizes for typical 32-bit and 64-bit programs. Integer data can be either *signed*, able to represent negative, zero, and positive values, or *unsigned*, only allowing nonnegative values. Data type `char` represents a single byte.

![Typical sizes (in bytes) of basic C data types.](C_data_size.png)

To avoid the vagaries of relying on "typical" sizes and different compiler settings, ISO C99 introduced a class of data types where the data sizes are fixed regardless of compiler and machine settings. Among these are data types `int32_t` and `int64_t`, having exactly 4 and 8 bytes, respectively. Using fixed-size integer types is the best way for programmers to have close control over data representations.

##### Addressing and Byte Ordering
```C
#include <stdio.h>

typedef unsigned char *byte_pointer;

void show_bytes(byte_pointer start, size_t len) {
    int i;
    for (i = 0; i < len; i++)
        printf(" %.2x", start[i]);
    printf("\n");
}
```
We use `typedef` to define data type `byte_pointer` as a pointer to an object of type `unsigned char`. Such a byte pointer references a sequence of bytes where each byte is considered to be a nonnegative integer. The first routine `show_bytes` is given the address of a sequence of bytes, indicated by a byte pointer, and a byte count. It prints the individual bytes in hexadecimal. The C formatting directive `%.2x` indicates that an integer should be printed in hexadecimal with at at least 2 digits.

##### Representing Strings
A string in C is encoded by an array of characters terminated by the null (having value 0) character. Each character is represented by some standard encoding, with the most common being the ASCII character code. Thus, if we run our routine `show_bytes` with arguments "`12345`" and 6 (to include the terminating character), we get the result `31 32 33 34 35 00`.

#### Integer Representations
##### Integral Data Types
C supports a variety of *integral* data types - ones that represent finite ranges of integers. Each type can specify a size with keyword `char`, `short`, `long`, as well as an indication of whether the represented number are all nonnegative (declared as `unsigned`), or possibly negative (the default). The number of bytes allocated for the different sizes varies according to whether the program is compiled for 32 or 64 bits.

One important feature to note is that the ranges are not symmetric - the range of negative numbers extends one further than the range of positive numbers. We will see why this happend

##### Unsigned Encodings
Let us consider an integer data type of \\( w \\)) bits. We write a bit vector as either \\( \vec{x} \\), to denote the entire vector, or as \\( [x_{w-1},x_{w-2},...,x_0] \\) to denote the individual bits within the vector. Treating \\( \vec{x} \\) as a number written in binary notation, we obtain the *unsigned* interpretation of \\( \vec{x} \\). In this encoding, each bit \\( x_i \\) has value 0 or 1, with the latter case indicating that value \\( 2^{i} \\) should be included as part of the numeric value. We can express this interpretation as a function \\( B2U_w \\) (for "binary to unsigned", length \\( w \\)):

For vector \\( \vec{x} = [x_{w-1},x_{w-2},\dotsc,x_0] \\):
\\( B2U_w(\vec{x}) = \displaystyle\sum_{i=0}^{w-1} x_i2^i \\)

In this equation, the notation \\( = \\) means that the left-hand side is defined to be equal to the right-hand side.

\\( B2U_4([1011]) =  1 \cdot 2^3 + 0 \cdot 2^2 + 1 \cdot 2^1 + 1 \cdot 2^0 = 11 \\)

##### Two's-Complement Encodings
For many applications, we wish to represent negative values as well. The most common computer representation of signed numbers is known as *two's-complement* form. This is defined by interpreting the most significant bit of the word to have negative weight. We express this interpretation as a function \\( B2T_w \\) (for "binary to two's complement", length  \\( w \\)):

For vector \\( \vec{x} = [x_{w-1},x_{w-2},\dotsc,x_0] \\):
\\( B2U_w(\vec{x}) = -x_{w-1}2^{w-1} + \displaystyle\sum_{i=0}^{w-2} x_i2^i \\)

The most significant bit \\( x_{w-1} \\) is also called the *sign bit*. Its "weight" is \\( -2^{w-1} \\), the negation of its weight in an unsigned representation. When the sign bit is set to 1, the represented value is negative, and when set to 0, the value is nonnegative.

\\( B2U_4([1011]) =  -1 \cdot 2^3 + 0 \cdot 2^2 + 1 \cdot 2^1 + 1 \cdot 2^0 = -5 \\)

##### Signed versus Unsigned in C
Adding character 'U' or 'u' as a suffix creates an unsigned constant; for example, `12345U` or `Ox1A2Bu`.

### Machine-Level Representation of Programs
#### Heterogeneous Data Structures
##### Unions
Unions provide a way to circumvent the type system of C, allowing a single object to be referenced according to multiple types. The syntax of a union declaration is identical to that of structures, but its semantics are very different. Rather than having the different fields reference different blocks of memory, they all reference the same block.

Consider the following declarations:
```C
union U3 {
    char c;
    int i[2];
    double v;
}
```
When compiled on an x86-64 Linux machine, the offsets of the fields, as well as the total size of data type `U3`, are as shown in the following table:

Type | c | i | v | Size
------------ | ------------- | ------------- | ------------- | -------------
U3 | 0 | 0 | 0 | 8

For pointer `p` of type `union U3 *`, references `p->c`, `p->i[0]`, and `p->v` would all references the beginning of the data structure. Observe also that the overall size of a union equals the maximum size of any of its fields.

#### Combining Control and Data in Machine-Level Programs
##### Thwarting Buffer Overflow Attacks
**Stack Randomization**

In order to insert exploit code into a system, the attacker needs to inject both the code as well as a pointer to this code as part of the attack string. Generating this pointer requires knowing the stack address where the string will be located. Historically, the stack address for a program were highly predictable. For all systems running the same combination of program and operating system version, the stack locations were fairly stable across many machines.

The idea of *stack randomization* is to make the position of the stack vary from one run of a program to another. Thus, even if many machines are running identical code, they would all be using different stack addresses. This is implemented by allocating a random amount of space between 0 and \\( n \\) bytes on the stack at the start of a program, for example, by using the allocation function `alloca`, which allocates space for a specified number of bytes on the stack. This allocated space is not used by the program, but it causes all subsequent stack locations to vary from one execution of a program to another. The allocation range \\( n \\) needs to be large enough to get sufficient variations in the stack address, yet small enough that it does not waste too much space in the program.

The following code shows a simple way to determine a "typical" stack address:
```
int main() {
    long local;
    printf("local at %p\n", &local);
    return 0;
}
```
This code simply prints the address of a local variable in the `main` function. Running the code 10000 times on a Linux machine in 32-bit mode, the address ranged from `0xff7fc59c` to `0xffffd09c`, a range of around \\( 2^{23} \\).

Stack randomization has become standard practice in Linux systems. It is one of a larger class technique known as *address-space layout randomization*, or ASLR. With ASLR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different regions of memory each time a program is run. That means that a program running on one machine will have very different address mappings than the same program running on other machines. This can thwart some forms of attack.

### Processor Architecture
So far, we have only viewed computer systems down to the level of machine language programs. We have seen that a processor must execute a sequence of instructions, where each instruction performs some primitive operation, such as adding two numbers. An instruction is encoded in binary form as a sequence of 1 or more bytes. The instructions supported by a particular processor and their byte-level encodings are known as *instruction set architecture* (ISA). Different "families" of processors, such as Intel IA32 and x86-64, IBM/Freescale Power, and the ARM processor family, have different ISAs. A program compiled for one type of machine will not run on another.

In this chapter, we take a brief look at the design of processor hardware. We study the way a hardware system can execute the instructions of a particular ISA. This view will give you a better understanding of how computers work and the technological challenges faced by computer manufactures. One important concept is that the actual way a modern processor operates can be quite different from the model of computation supplied by the ISA. The ISA model would seem to imply *sequential* instruction execution, where each instruction is fetched and executed to completion before the next one begins.

In this chapter, we start by defining a simple instruction set that we use as a running example for our processor implementations. We call this the "Y86-64" instruction set, because it was inspired by the x86-64 instruction set. Compared with x86-64, the Y86-64 instruction set has fewer data types, instructions, and addressing modes.

As a first step in designing a processor, we present a functionally correct, but somewhat impractical, Y86-64 processor based on *sequential* operation. This processor executes a complete Y86-64 instruction on every clock cycle.

#### The Y86-64 Instruction Set Architecture
##### Programmer-Visible State
As Figure 4.1 illustrates, each instruction in a Y86-64 program can read and modify some part of the processor state. This is referred to as the *programmer-visible* state, where the "programmer" in this case is either someone writing programs in assembly-code or a compiler generating machine-level code. The state for Y86-64 is similar to that for x86-64. There are 15 *program registers*: `%rax`, `%rcx`, `%rdx`, `%rbx`, `%rsp`, `%rbp`, `%rsi`, `%rdi`, and `%r8` through `%r14`. Each of these stores a 64-bit word. The program counter (PC) holds the address of the instruction currently being executed.

The *memory* is conceptually a large array of bytes, holding both program and data. Y86-64 programs reference memory locations using *virtual address*. A combination of hardware and operating system software translates these into the actual, or *physical*, address indicating where the values are actually stored in memory.

A final part of the program state is a status code `Stat`, indicating the overall state of the program execution. It will indicate either normal operation or some sort of *exception* has occurred, such as an instruction attempts to read from an individual memory address.

##### Y86-64 Instructions
Figure 4.2 gives a description of the individual instructions in the Y86-64 ISA. We use this instruction set as a target for our processor implementations. In this figure, we show the assembly-code representation of the instructions on the left and the byte encodings on the right. The assembly-code format is similar to the ATT format for x86-64.

![Y86-64 instruction set.](y86-64_instruction_set.png)

Here are some details about the Y86-64 instructions.

- The x86-64 `movq` instruction is split into four different instructions: `irmovq`, `rrmovq`, `mrmovq`, and `rmmovq`, explicitly indicating the form of the source and destination. The source is either immediate (`i`), register (`r`), or memory (`m`). It is designated by the first character in the instruction name. The destination is either register (`r`) or memory (`m`). Is is designated by the second character in the instruction name.
  As with x86-64, we do not allow direct transfer from one memory location to another. In addition, we do not allow a transfer of immediate data to memory.
- There are four integer operation instructions, as shown in Figure 4.2 as `OPq`. These are `addq`, `subq`, `andq`, and `xorq`. They operate only on register data, whereas x86064 allows operations on memory data.
- The `call` function pushes the return address on the stack and jumps to the destination address. The `ret` instruction returns from such a call.
- The `pushq` and `popq` instructions implement push and pop, just as they do in x86-64.

#### Instruction Encoding
Figure 4.2 shows the byte-level encodings of the instructions. Each instruction requires between 1 and 10 bytes, depending on which fields are required. Every instruction has an initial byte identifying the instruction type.

As shown in Figure 4.4, each of the 15 program register has an associated *register identifier* (ID) ranging from `0` to `0xE`. The program registers are stored within the CPU in a *register file*, a small random access memory where the register IDs serve as addresses.

Number | Register name
------------ | -------------
0 | `%rax`
1 | `%rcx`
2 | `%rdx`
3 | `%rbx`
4 | `%rsp`
5 | `%rbp`
... | ...

Some instructions are just 1 byte long, but those that require operands have longer encodings. First, there can be an additional *register specifier type*, specifying either one or two registers. These registers are called `rA` and `rB` in Figure 4.2. As the assembly-code versions of the instructions show, they can specify the registers used for data sources and destinations, as well as the base register used in an address computation, depending on the instruction type. Instructions that have no operands, such as branches and `call`, do not have a register specifier byte.

Some instructions require an additional 8-byte *constant word*. This word can serve as the immediate data for `irmovq`, the displacement for `rmmovq` and `mrmovq` address specifiers, and the destination of branches and calls.

As an example, let us generate the byte encoding of the instruction `rmmovq %rsp, 0x123456789abcd(%rdx)` in hexadecimal. From Figure 4.2, we can see that `rmmovq` has initial byte 40. We can also see that source register `%rsp` should be encoded in the `rA` field, and base register `%rdx` should be encoded in the `rB field`. Using the register numbers in Figure 4.4, we get a register specifier type of 42. Finally, the displacement is encoded in the 8-byte constant word. We first pad `0x123456789abcd` with leading zeros to fill out 8 bytes, giving a byte sequence of `00 01 23 45 67 89 ab cd`. We rewrite this in byte-reversed order as `cd ab 89 67 45 23 01 00`. Combining this, we get an instruction encoding of `4042cdab896745230100`.

One important property of any instruction set is that the byte encodings must have a unique interpretation.

#### Sequential Y86-64 Implementations
Now we have the components required to implement a Y86-64 processor. As a first step, we describe a processor called SEQ (for "sequential" processor). On each clock cycle, SEQ performs all the steps required to process a complete instruction. This would require a very long cycle time, however, and so the clock rate would be unacceptable low.

##### Organizing Processing into Stages
In general, processing an instruction involves a number of operations. We organize them in a particular sequence of stages, attempting to make all instructions follow a uniform sequence, even though the instructions differ greatly in their actions.

- *Fetch.* The fetch stage reads the bytes of an instruction from memory, using the program counter (PC) as the memory address. From the instruction it extracts the two 4-bit portions of the instruction specifier type, referred to as `icode` (the instruction code) and `ifun` (the instruction function). It possibly fetches a register specifier byte, giving one or both the of register operands specifiers `rA` and `rB`. It also possibly fetches an 8-byte constant word `valC`. It computes `valP` to be address of the instruction following the current one in sequential order. That is, `valP` equals the value of the PC plus the length of the fetched instruction.
- *Decode.* The decode stage reads up to two operands from the register file, giving values `valA` and/or `valB`. Typically, it reads the registers designated by instruction fields `rA` and `rB`,but for some instructions it reads register `%rsp`.
- *Execute.* In the execute stage, the arithmetic logic unit (ALU) either performs the operation explicitly specified by the instruction (according to the value of `ifun`), computes the effective address of a memory reference, or increments or decrements the stack pointer. We refer to the resulting value as `valE`.
- *Memory.* The memory stage may write data to memory, or it may read data from memory.
- *Write back.* The write-back stage writes up to two results to the register file.
- *PC update.* The PC is set to the address of the next instruction.

The processor loops indefinitely, performing these stages. In our simplified implementation, the processor will stop when any exception occurs - that is, when it executes a `halt` or invalid instruction, or it attempts to read or write an invalid address. In a more complete design, the processor would enter an exception-handling mode and begin executing special code determined by the type of exception.

Figure 4.18 shows the processing required for instruction types `OPq` (integer and logical operations), `rrmovq` (register -register move), and `irmovq` (immediate-register move). Let us consider the integer operations.

![Computations in sequential implementation of Y86-64 instruction OPq, rrmovq, and irmovq.](y86-64_sequential_pushq.png)

The processing of an integer-operation instruction follows the general pattern listed above. In the fetch stage, we do not require a constant word, and so `valP` is computed as `PC + 2`. During the decode stage, we read both operands. These are supplied to the ALU in the execute stage, along with the function specifier `ifun`, so that `valE` becomes the instruction result. This computation is shown as the expression `valB OP valA`, where `OP` indicates the operation specified by `ifun`. Nothing happens in the memory stage for these instructions, but `valE` is written to register `rB` in the write-back stage, and the PC is set to `valP` to complete the instruction execution.

Figure 4.19 shows the processing required for the memory write and read instruction `rmmovq` and `mrmovq`. We see the same basic flow as before, but using the ALU to add `valC` to `valB`, giving the effective address (the sum of the displacement (offset) and the base register value (start of memory region)) for the memory operation. In the memory stage, we either write the register value `valA` to memory or read `valM` from memory.

![Computations in sequential implementation of Y86-64 instruction rmmovq and mrmovq.](y86-64_sequential_rmmovq.png)

The `pushq` instruction starts much like our previous instructions, but in the decode stage we use `%rsp` as the identifier for the second register operand, giving the stack pointer as value `valB`. In the execute stage, we use the ALU to decrement the stack pointer by 8. This decremented value is used for the memory write address and is also stored back to `%rsp` in the write-back stage.

![Computations in sequential implementation of Y86-64 instruction rmmovq and mrmovq.](y86-64_sequential_call.png)

Instructions `call` and `ret` bear some similarity to instructions `pushq` and `popq`, except that we push and pop program counter values. With instruction `call`, we push `valP`, the address of of the instruction that follows the `call` instruction. During the PC update stage, we set the `PC` to `valC`, the call destination.

### Optimizing Program Performance
#### Capabilities and Limitations of Optimizing Compilers
Modern compilers employ sophisticated algorithms to determine what values are computed in a program and how they are used. They can then exploit opportunities to simplify expressions, to use a single computation in several different places, and to reduce the number of times a given computation must be performed. Most compilers, including GCC, provide users with some control over which optimizations they apply. As discussed in Chapter 3, the simplest control is to specify the *optimization level*. For example, invoking GCC with the command-line option `-Og` specifies that it should apply a basic set of optimizations.

Invoking GCC with option `-O1` or higher (e.g., `-O2` or `-O3`) will cause it to apply more extensive optimizations. These can further improve program performance, but they may extend the program size and they may make the program more difficult to debug using standard debugging tools. For our presentation, we will mostly consider code compiled with optimization level `-O1`, even though level `-O2` has become the accepted standard for most software projects that use GCC.

Compilers must be careful to apply only *safe* optimizations to a program, meaning that the resulting program will have the exact same behavior as would an unoptimized version for all possible cases the program may encounter, up to the limits of the guarantees provided by the C language standards. Constraining the compiler to perform only safe optimizations eliminates possible sources of undesired run-time behavior, but it also means that the programmer must make more of an effort to write programs in a way that the compiler can then transform into efficient machine-level Â·code. To appreciate the challenges of deciding which program transformations are safe or not, consider the following two procedures:
```
 1    void twiddle1(long *xp, long *yp)
 2    {
 3        *xp += *yp;
 4        *xp += *yp;
 5    }
 6
 7    void twiddle2(long *xp, long *yp)
 8    {
 9        *xp += 2* *yp;
10    }
```
At fist glance, both procedures seem to have identical behavior. They both add twice the value stored at the location designated by pointer `yp` to that designated by pointer `xp`. On the other hand, function `twiddle2` is more efficient. It requires only three memory references (read `*xp`, read `*yp`, write `*xp`), whereas `twiddle1` requires six (two reads of `*xp`, two reads of `*yp`, and two writes of `*xp`). Hence, if a compiler is given procedure `twiddle1` to compile, one might think it could generate more efficient code based on the computations performed by `twiddle2`.

```
1    long f();
2
3    long func1() {
4        return f() + f() + f() + f();
5    }
```
```
1    int counter = 0;
2
3    int f() {
4        return counter++;
5    }
```
Code involving function calls can be optimized by a process known as *inline substitution* (or simply "inlining"), where the function call is replaced by the code for the body of the function. For example, we can expand the code for `func1` by substituting four instantiations of function `f`:
```
1    /* Result of inlining f in func1 */
2    long func1in() {
3        long t = counter++; /* +0 */
4        t += counter++;     /* +1 */
5        t += counter++;     /* +2 */
6        t += counter++;     /* +3 */
7        return t;
8    }
```
This transformation both reduces the overhead of the function calls and allow further optimization of the expanded code. For example, the compiler can consolidate the updates of global variable `counter` in `func1in` to generate an optimized version of the function:
```
1    /* Optimization of inline code */
2    long func1opt() {
3        long t = 4 * counter + 6;
4        counter += 4;
5        return t;
6    }
```
This code faithfully reproduces the behavior of `func1` for this particular definition of function `f`.

Recent versions of GCC attempt this form of optimization, either when directed with the command-line option `-finline` or for optimization level `-O1` and higher.

There are times when it is best to prevent a compiler from performing inline substitution. One is when the code will be evaluated using a symbolic debugger, such as GDB. If a function call has been optimized away via inline substitution, then any attempt to trace or set a breakpoint for that call will fail. The second if when evaluating the performance of a program by profiling. Calls to functions that have been eliminated by inline substitution will not be profiled correctly.

#### Identifying and Eliminating Performance Bottlenecks
##### Program Profiling
Program *profiling* involves running a version of a program in which instrumentation code has been incorporated to determine how time the different parts of the program require. It can be very useful for identifying the parts of a program we should focus on in our optimization efforts. One strength of the profiling is that it can be performed while running the actual program on realistic benchmark data.

Unix systems provide the profiling program `GRPOF`. This program generates two forms of information. First, it determines how much CPU time was spent for each of the functions in the program. Second, it computers a count of how many times each function gets called, categorized by which function performs the call. Both forms of information can be quite useful The timings give a sense of the relative importance of the different functions in determining the overall run time. The calling information allows us to understand the dynamic behavior of the program.

Profiling with `GPROF` requires three steps, as shown for a C program `prog.c`, which runs with command-line argument `file.txt`:

1. The program must be compiled and linked for profiling. With GCC (and other C compilers), this involves simply including the run-time flag `-pg` on the command line. It is important to ensure that the compiler does not attempt to perform any optimizations via inline substitution, or else the calls to functions may not be tabulated accurately. We use optimization flag `-Og`, guaranteeing that function calls be tracked properly.
  ```
  linux> gcc -Og -pg prog.c -o prog
  ```
2. The program is then executed as usual (`linux> ./prog file.txt`).
  It runs slightly (around a factor of 2) slower than normal, but otherwise the only difference is that it generates a file `gmon.out`.
3. `GPROF` is invoked to analyze the data in `gmon.out`:
  ```
  linux> gprof prog
  ```

The first part of the profile report lists the times spent executing the different functions, sorted in descending order.
```C
/* Recursively find string in list.  Add to end if not found */
h_ptr find_ele_rec(h_ptr ls, char *s)
{
    if (!ls) {
        /* Come to end of list.  Insert this one */
        ucnt++;
        return new_ele(save_string(s));
    }
    if (strcmp(s,ls->word) == 0) {
        ls->freq++;
        if (ls->freq > mcnt) {
            mcnt = ls->freq;
            mstring = ls->word;
        }
        return ls;
    }
    ls->next = find_ele_rec(ls->next, s);
    return ls;
}

void insert_string(char *s,
           hash_fun_t hash_fun, lower_fun_t lower_fun,
           find_ele_fun_t find_ele_fun)
{
    int index;
    lower_fun(s);
    index = hash_fun(s);
    htable[index] = find_ele_fun(htable[index], s);  
}
```
As an example, the following listing shows this part of the report for the three most time-consuming functions in a program:
```
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 97.58    203.66   203.66        1   203.66   203.66  sort_words
  2.32    208.50     4.85   965027     0.00     0.00  find_ele_rec
  0.14    208.81     0.30 12511031     0.00     0.00  Strlen
```
Each row represents the time spent for all calls to some function. The first column indicates the percentage of the overall time spent on the function. The second shows the cumulative time spent by the functions up to and including the one on this row. The third row shows the time spent on this particular function, and the fourth shows how many times it was called (not counting recursive calls). In our example, the function `sort_words` was called only once, but this single call required 203.66 seconds, while the function `find_ele_rec` was called 965027 times (not including recursive calls), requiring a total of 4.85 seconds. Function `Strlen` computes the length of a string by calling the library function `strlen`. Library function calls are normally not shown in the results of `GPROF`. Their times are usually reported as part of the function calling them. By creating the "wrapper function" `Strlen`, we can reliably track the calls to `strlen`, showing that it was called 12511031 times but only requiring a total of 0.30 seconds.

The second part of the profile report shows the calling history of the functions. The following is the history for a recursive function `find_ele_rec`:
```
                             158655725             find_ele_rec [5]
                4.85    0.10  965027/965027      insert_string [4]
[1]     52.4    4.85    0.10  965027+158655725 find_ele_rec [5]
                0.08    0.01  363039/363039      save_string [8]
                0.00    0.01  363039/363039      new_ele [12]
                4.85    0.01 158655725             find_ele_rec [5]
```
The history shows both the functions that called `find_ele_rec`, as well as the functions that it called. The first two lines show the calls to the functions: 158655725 calls by itself recursively, and 965027 calls by function `insert_string` (which is itself called 965027 times). Function `find_ele_rec`, in turn, called two other functions, `save_string` and `new_ele`, each a total of 363039 times.

From these call data, we can often infer useful information about the program behavior. For example, the function `find_ele_rec` is a recursive procedure that scans the linked list for a hash bucket looking for a particular string. For this function, comparing the number of recursive calls with the number of top-level calls provides statistical information about the lengths of the traversal through these lists. Given that their ratio is 164.4:1, we can infer that the program scanned an average of around 164 elements each time.

### The Memory Hierarchy
#### Storage Technologies
##### Random Access Memory
**Nonvolatile Memory**

DRAMs and SRAMs are *volatile* in the sense that they lose their information if the supply voltage is turned off. *Nonvolatile memories*, on the other hand, retain their information even when they are powered off. There are a variety of nonvolatile memories. For historical reasons, they are referred to collectively as *read-only memories* (ROMs), even though some types of ROMs can be written to as well as read. ROMs are distinguished by the number of times they can be reprogrammed (written to) and by the mechanism for reprogramming them.

A *programmable ROM (PROM)* can be programmed exactly once. PROMs include a sort of fuse with each memory cell that can be blown once by zapping it with a high current.

An *erasable programmable ROM (EPROM)* has a transparent quartz window that permits light to reach the storage cells. The EPROM cells are cleared to zeros by shinning ultraviolet light though the window. Programming an EPROM is done by using a special device to write ones into the EPROM. An EPROM can be erased and reprogrammed on the order of 1000 times. An *electrically erasable PROM (EEPROM)* is akin to an EPROM, but it does not require a physically separate programming device, and thus can be reprogrammed in-place on printed circuit cards. An EEPROM can be reprogrammed on the order of 100000 times before it wears out.

*Flash memory* is a type of nonvolatile memory, based on EEPROMs, that has become an important storage technology. We will look in detail at a new form of flash-based disk drive, known as a *solid state disk (SSD)*, that provides a faster, sturdier, and less power-hungry alternative to conventional rotating disks.

##### Disk Storage
*Disks* are workhorse storage devices that hold enormous amounts of data, on the order of hundreds to thousands of gigabytes, as opposed to the hundreds or thousands of megabytes in a RAM-based memory.

##### Solid State Disks
A solid state disk (SSD) is a storage technology, based on flash memory, that in some situations is an attractive alternative to the conventional rotating disk.

![Solid state disk (SSD).](ssd.png)

An SSD package plugs into a standard disk slot on the I/O bus (typically USB or SATA) and behaves like any other disk, processing requests from the CPU to read and write logical disk blocks. An SSD package consists of one or more flash memory chips, which replace the mechanical driver in a conventional rotating disk, and a *flash translation layer*, which is a hardware/firmware device that plays the same role as a disk controller, translating requests for logical blocks into accesses of the underlying physical device.

A flash memory consists of a sequence of \\( B \\) *blocks*, where each block consists of \\( P \\) pages. Typically, pages are 512 bytes to 4KB in size, and a block consists of 32-128 pages, with total block sizes ranging from 16KB to 512KB. Data are read and written in units of pages. A page can be written only after the entire block to which it belongs has been *erased* (typically, this means that all bits in the block are set to 1). However, once a block is erased, each page in the block can be written once with no further erasing. A block wears out after roughly 100000 repeated writes. Once a block wears out, it can no longer be erasing.

Random writes are slower for two reasons. First, erasing a block takes a relatively long time, on the order of 1ms, which is more than an order to magnitude longer than it takes to access a page. Second, if a write operation attempts to modify a page \\( p \\) that contains existing data (i.e., not all ones), then any pages in the same block with useful data must be copied to a new (erased) block before the write to page \\( p \\) can occur.

SSDs have a number of advantages over rotating disks. They are built of semiconductor memory, with no moving parts, and thus have much faster random access times than rotating disks, use less power, and are more rugged.

#### Cache Memories
The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main memory, and disk storage. However, because of the increasing gap between CPU and main memory, system designers were compelled to insert a small SRAM *cache memory*, called an *L1 cache* (level 1 cache) between the CPU register file and main memory. The L1 cache can be accessed nearly as fast as the registers, typically in about 4 clock cycles.

![Typical bus structure for cache memories.](cache_memories.png)

As the performance gap between the CPU and main memory continued to increase, system designers responded by inserting an additional larger cache, called an *L2 cache*, between the L1 cache and main memory, that can be accessed in about 10 clock cycles. Many modern systems include an even larger cache, called an *L3 cache*, which sits between the L2 cache and main memory in the memory hierarchy and can be accessed in about 50 cycles.

##### Generic Cache Memory Organization
A cache for such a machine is organized as an array of *cache sets*. Each set consists of *E* *cache lines*. Each cache line consists of a data block of \\( B = 2^b \\) bytes, a *valid bit* that indicates whether or not the line contains meaningful information, and \\( t = m - (b + s) \\) tag bits (a subset of the bits from the current block's memory address) that uniquely identify the block stored in the cache line.

In general, a cache's organization can be characterized by the tuple (\\( S \\), \\( E \\), \\( B \\), \\( m \\)). The size (or capacity) of a cache, \\( C \\), is stated in terms of the aggregate size of all the blocks. Thus, \\( c = S \times E \times B \\).

When the CPU is instructed by a load instructor to read a word from address \\( A \\) of main memory, it sends address \\( A \\) to the cache. If the cache is holding a copy of the word at address \\( A \\), it sends the word immediately back to the CPU. So how does the cache know whether it contains a copy of the word at address \\( A \\)? The cache is organized so that it can find the requested word by simply inspecting the bits of the address, similar to a hash table with an extremely simple hash function.

##### Issues with Writes
As we have seen, the operation of a cache with respect to reads is straightforward. First, look for a copy of the desired word \\( w \\) in the cache. If there is a hit, return \\( w \\) immediately. If there is a miss, fetch the block that contains \\( w \\) from the next lower level of the memory hierarchy, store the block in some cache line (possibly evicting a valid line), and then return \\( w \\).

The situation for writes is a little more complicated. Suppose we write a word \\( w \\) that is already cached (a *write hit*). After the cache updates its copy of \\( w \\), what does it do about updating the copy of \\( w \\) in th next lower level of the hierarchy? The simplest approach, known as *write-through*, is to immediately write \\( w \\)'s cache block to the next lower level. While simple, write-through has the disadvantage of causing bus traffic with every write. Another approach, known as *write-back*, defers the update as long as possible by writing the updated block to the next lower level when it is evicted from the cache by the replacement algorithm. Because of locality, write-back can significantly reduce the amount of bus traffic, but it has the advantage pf additional complexity. The cache must maintain an additional *dirty bit* for each cache line that indicates whether or not the cache block has been modified.

## Running Programs on a System
### Linking
#### Compiler Drivers
```
(a)main.c                                              (b)sum.c
-------------------------------- code/link/main.c      -------------------------------- code/link/sum.c

1    int sum(int *a, int n)                            1    int sum(int *a, int n)
2                                                      2    {
3    int array[2] = {1, 2};                            3        int i, s = 0;  
4                                                      4
5    int main()                                        5        for (i = 0; i < n; i++) {
6    {                                                 6            s += a[i];
7        int val = sum(array, 2);                      7        }
8        return val;                                   8        return s;
9    }                                                 9    }

-------------------------- code/link/main.c            -------------------------------- code/link/sum.c
```
Most compilation systems provide a *compile driver* that invokes the language preprocessor, compiler, assembler, and linker, as needed on the behalf of the user. For example, to build the example program using the GNU compilation system, we might invoke the GCC driver by typing the following command to the shell:
```
linux> gcc -Og -o prog main.c sum.c
```
The driver first runs the C preprocessor (`cpp`), which translates the C source file `main.c` into an ASCII intermediate file `main.i`:
```
cpp [other arguments] main.c /tmp/main.i
```
Next, the driver runs the C compiler (`cc1`), which translates `main.i` into an ASCII assembly-language file `main.s`:
```
cc1 /tmp/main.i -Og [other arguments] -o /tmp/main.s
```
Then, the driver runs the assembler (`as`), which translates `main.s` into a binary *relocatable object file* `main.o`:
```
as [other arguments] -o /tmp/main.o /tmp/main.s
```
The driver goes through the same process to generate `sum.o`. Finally, it runs the linker program `ld`, which combines `main.o` and `sum.o`, along with the necessary system object files, to create the binary *executable object file* `prog`:
```
ld -o prog [system object files and args] /tmp/main.o /tmp/sum.o
```
To run the executable `prog`, we type its name on the Linux shell's command line:
```
linux> ./prog
```
The shell invokes a function in the operating system called the *loader*, which copies the code and data in the executable file `prog` into memory, and then transfers control to the beginning of the program.

#### Relocatable Object Files
.text: The machine code of the compiled program.

.data: *Initialized* global and static C variables. Local C variables are maintained at run time on the stack, and do *not* appear in either the .data or .bss sections.

.bss: *Uninitialized* global and static C variables, along with any global or static variables that are initialized to zero.

.symtab: A *symbol table* with information about functions and global variables that are defined and referenced in the program.

#### Symbols and Symbol Tables
Each relocatable object module, \\( m \\), has a symbol table that contains information about the symbols defined and referenced by \\( m \\). In the context of a linker, there are three different kinds of symbols:
- *Global symbols* that are defined by module \\( m \\) and that can be referenced by other modules.
- Global symbols that are referenced by module \\( m \\) but defined by some other module.
Such symbols are called *externals* and correspond to nonstatic C functions and global variables that are defined in other modules.
- *Local symbols* that are defined and referenced exclusively by module \\( m \\).

Symbol tables are built by assemblers, using symbols exported by the compiler into the assembly language. An ELF symbol table is contained in the `.symtab` section. It contains an array of entries.

The GNU `READELF` program is a handy tool for viewing the contents of object files: For example, here are the last three symbol table entries for the relocatable object file `main.o`. The first eight entries, which are not shown, are local symbols that the linker uses internally.
```
Num:    Value          Size Type    Bind   Vis      Ndx Name
  8: 0000000000000000    24 FUNC    GLOBAL DEFAULT    1 main
  9: 0000000000000000     8 OBJECT  GLOBAL DEFAULT    3 array
 10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND sum
```
In this example, we see an entry for the definition of global symbol `main`, a 24-byte function located at an offset (i.e., `value`) of zero in the `.text` section. This is followed by the definition of the global symbol `array`, an 8-byte object located at an offset of zero in the `.data` section. The last entry comes from the reference to the external symbol `sum`. `READELF` identifies each section by an integer index. `Ndx=1` denotes the `.text` section, and `Ndx=3` denotes the `.data` section.

#### Symbol Resolution
Resolving references to global symbols, however, is trickier. When the compiler encounters a symbol (either a variable or function name) that is not defined in the current module, it assumes that it is defined in some other module, generates a linker symbol table entry, and leaves it for the linker to handle. If the linker is unable to find a definition for the referenced symbol in any of input modules, it prints an error message and terminates. For example, if we try to compile and link the following source file on a Linux machine,
```
void foo(void);

int main() {
	foo();
	return 0;
}
```
then the compiler runs without a hitch, but the linker terminates when it cannot resolve the reference to `foo`:
```
linux> gcc -Wall -Og -o linkerror linkerror.c 
/tmp/ccSz5uti.o: In function `main':
/tmp/ccSz5uti.o(.text+0x7): undefined reference to `foo'
```
Symbol resolution for global symbols is also tricky because the same symbol might be defined by multiple object files. In this case, the linker must either flag an error or somehow choose one of the definitions and discard the rest.

##### How Linkers Resolve Duplicate Global Symbols
At compile time, the compiler exports each global symbol to the assembler as either *strong* or *weak*, and the assembler encodes this information implicitly in the symbol table of the relocatable object file. Functions and initialized global variables get strong symbols. Uninitialized global variables get weak symbols.

Given this notion of strong and weak symbols, Linux linkers use the following rules for dealing with multiply duplicate symbol names:
- Rule 1: Multiple strong symbols with the same name are not allowed.

For example, suppose we attempt to compile and link the following two C modules:
```
1    /* foo1.c */
2    int main()
3    {
4        return 0;
5    }

1    /* bar1.c */
2    int main()
3    {
4        return 0;
5    }
```
In this case, the linker will generate an error message because the strong symbol `main` is defined in multiple times (rule 1):
```
linux> gcc foo1.c bar1.c
/tmp/ccq2Uxnd.o: In function `main':
/tmp/ccq2Uxnd.o(.text+0x0): multiple definition of `main'
```

##### Linking with Static Libraries
```
(a)addvec.o                                           (b)multvec.c
----------------------------- code/link/addvec.c      ----------------------------- code/link/multvec.c

 1    int addcnt = 0;                                  1    int multcnt = 0;
 2                                                     2
 3    void addvec(int *x, int *y,                      3    void multvec(int *x, int *y,
 4                int *z, int n)                       4                 int *z, int n)
 5    {                                                5    {
 6        int i;                                       6        int i;
 7                                                     7
 8        addcnt++;                                    8        multcnt++;
 9                                                     9
10        for (i = 0; i < n; i++)                     10        for (i = 0; i < n; i++)
11            z[i] = x[i] + y[i];                     11            z[i] = x[i] * y[i];
12    }                                               12    }
----------------------------- code/link/addvec.c      ----------------------------- code/link/multvec.c
```

To create a static library of these functions, we would use the `AR` tool as follows:
```
linux> gcc -c addvec.c multvec.c
linux> ar rcs libvector.a addvec.o multvec.o
```
To build the executable, we would compile and link the input file `main2.o` and `libvector.a`:
```
linux> gcc -c main2.c
linux> gcc -static -o prog2c main2.o ./libvector.a
```
Or equivalently,
```
linux> gcc -c main2.c
linux> gcc -static -o prog2c main2.o -L. -lvector
```
The `-static` argument tells the compiler driver that the linker should build a fully linked executable object file that can be loaded into memory and run without any further linking at load time. The `-lvector` argument is a shorthand for `libvector.a`, and the `-L.` argument tells the linker to look for `libvector.a` in the current directory.

When the linker runs, it determines that the `addvec` symbol defined by `addvec.o` is referenced by `main.o`, so it copies `addvec.o` into the executable. Since the program doesn't reference any symbols defined by `multvec.o`, the linker does not copy this module into the executable. The linker also copies `printf.o` module from `libc.a`, along with a number of other modules from the C run-time system.

##### How Linkers Use Static Libraries to Resolve References
During the symbol resolution phase, the linker scans the relocatable object files and archives left to right in the same sequential order that they appear on the compiler driver's command line. During this scan, the linker maintains a set \\( E \\) of relocatable object files that will be merged to form the executable, a set \\( U \\) of unresolved symbols, and a set \\( D \\) of symbols that have been defined in previous input files.
- For each input file \\( f \\) on the command line, the linker determines if \\( f \\) is an object file or an archive. If \\( f \\) is an object file, the linker adds \\( f \\) to \\( E \\), updates \\( U \\) and \\( D \\) to reflect the symbol definitions and references in \\( f \\), and proceeds to the next input file.
- If \\( f \\) is an archive, the linker attempts to match the unresolved symbols in \\( U \\) against the symbols defined by the members of the archive. If some archive member, \\( m \\), defines a symbol that resolves a reference in \\( U \\), then \\( m \\) is added to \\( E \\), and the linker updates \\( U \\) and \\( D \\) to reflect the symbol definitions and references in \\( m \\). This process iterates over the member object files in the archive until a fixed point is reached where \\( U \\) and \\( D \\) no longer change. At this point, any member object files not contained in \\( E \\) are simply discarded and the linker proceeds to the next input file.
- If \\( U \\) is nonempty when the linker finishes scanning the input files on the command line, it prints an error and terminates. Otherwise, it merges and relocates the object files in *E* to build the output executable file.

If the library that defines a symbol appears on the command line before the object file that references that symbol, then the reference will not be resolved and linking will fail. For example, consider the following:
```
linux> gcc -static ./libvector.a main2.c
```
When `libvector.a` is processed, \\( U \\) is empty, so no member object files from `libvector.a` are added to \\( E \\). Thus, the reference to `addvec` is never resolved and the linker emits an error message and terminates.

GNU linker option: `--as-needed` and `--no-as-needed`

This option affects ELF `DT_NEEDED` tags for **dynamic libraries** mentioned on the command line after the `--as-needed` option. Normally the linker will add a `DT_NEEDED` tag for each dynamic library mentioned on the command line, regardless of whether the library is actually needed or not. `--as-needed` causes a `DT_NEEDED` tag to only be emitted for a library that at that point in the link satisfies a non-weak undefined symbol reference from a regular object file or, if the library is not found in the `DT_NEEDED` lists of other libraries, a non-weak undefined symbol reference from another dynamic library. Object files or libraries appearing on the command line after the library in question do not affect whether the library is seen as needed. `--no-as-needed` restores the default behaviour.

#### Relocation
Once the linker has completed the symbol resolution step, it has associated each symbol reference in the code with exactly one symbol definition (i.e., a symbol table entry in one of its input object modules). At this point, the linker knows the exact sizes of the code and data sections in its input object modules. It is now ready to begin the relocation step, where it merges the input modules and assigns run-time addresses to each symbol. Relocation consists of two steps:
1. *Relocating sections and symbol definitions.* In this step, the linker merges all sections of the same type into a new aggregate section of the same type. For example, the `.data` sections from the input modules are all merged into one section that will become the `.data` section for the output executable object file. The linker then assigns run-time memory addresses to the new aggregate sections, to each section defined by the input modules, and to each symbol defined by the input modules. When this step is complete, each instruction and global variable in the program has a unique run-time memory address.
2. *Relocating symbol references within sections.* In this step, the linker modifies every symbol reference in the bodies of the code and data sections so that they point to the correct tun-time address.

##### Relocation Entries
When an assembler generates an object module, it does not know where the code and data will ultimately be stored in memory. Nor does it know the locations of any externally defined functions or global variables that are referenced by the module. So whenever the assembler encounters a reference to an object whose ultimate location is unknown, it generates a *relocation entry* that tells the linker how to modify the reference when it merges the object file into an executable.

#### Executable Object Files
We have seen how the linker merges multiple object files into a single executable object file.

The format of an executable object file is similar to that of a relocatable object file. The ELF header describes the overall format of the file. It also includes the program's *entry point*, which is the address of the first instruction to execute when the program runs. The `.text`, `.rodata`, and `.data` sections are similar to those in a relocatable object file, except that these sections have been relocated to their eventual run-time memory addresses.

#### Loading Executable Object Files

To run an executable object file `prog`, we can type its name to the Unix shell's command line:
```
linux> ./prog
```
Since `prog` does not correspond to a built-in shell command, the shell assumes that `prog` is an executable object file, which it runs for us by invoking some memory-resident operating system code known as the loader. The loader copies the code and data in the executable object file from disk into memory, and then runs the program by jumping to its first instruction, or *entry point*. This process of copying the program into memory and then running it is known as *loading*.

![Linux x86-64 run-time memory image.](run-time_memory.png)

#### Dynamic Linking with Shared Libraries
Another issue is that almost every C program uses standard I/O functions such as `printf` and `scanf`. At run time, the code for these functions is duplicated in the text segment of each running process. On a typical system that is running hundreds of processes, this can be a significant waste of scarce memory system resources.

*Shared libraries* are modern innovations that address the disadvantages of static libraries. A shared library is an object module that, at run time, can be loaded at an arbitrary memory address and linked with a program in memory. This process is known as *dynamic linking* and is performed by a program called a *dynamic linker*.

Shared libraries are "shared" in two different ways. First, in any given file system, there is exactly only one `.so` file for a particular library. The code and data in this `.so` file are shared by all of the executable object files that reference the library, as opposed to the contents of static libraries, which are copied and embedded in the executables that reference them. Second, a single copy of the `.text` section of a shared library in memory can be shared by different running processes.

To build a shared library `libvector.so` of our example vector routines, we would invoke the compiler driver with the following special directive to the compiler and linker:
```
linux> gcc -shared -fpic -o libvector.so addvec.c multvec.c
```
The `-shared` flag directs the linker to create a shared object file. Once we have created the library, we would then link it into our example program:
```
linux> gcc -o prog21 main2.c ./libvector.so
```
This creates an executable object file `prog21` in a form that can be linked with `libvector.so` at run time. The basic idea is to do some of the linking statistically when the executable file is created, and then complete the linking process dynamically when the program is loaded. It is important to realize that none of the code or data sections from `libvector.so` are actually copied into the executable `prog21` at this point.

When the loader loads and runs the executable `prog21`, it loads the partially linked executable `prog21`. Next it notices that `prog21` contains a `.interp` section, which contains the name of the dynamic linker, which is itself a shared object (e.g., `ld-linux.so` on Linux systems). Instead of passing control to the application, as it would normally do, the loader loads an runs the dynamic linker. The dynamic linker then finishes the linking task by performing the following relocations:
- Relocating the text and data of `libc.so` into some memory segment.
- Relocating the text and data of `libvector.so` into another memory segment.
- Relocating any references in `prog21` to symbols defined by `libc.so` and `libvector.so`.

```
$ cat /proc/3870/maps
# before addvec() is invoked
...
7f63ea16a000-7f63ea326000 r-xp 00000000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea326000-7f63ea525000 ---p 001bc000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea525000-7f63ea529000 r--p 001bb000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea529000-7f63ea52b000 rw-p 001bf000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea52b000-7f63ea530000 rw-p 00000000 00:00 0
7f63ea530000-7f63ea531000 r-xp 00000000 08:02 23069711                   ./libvector.so
7f63ea531000-7f63ea730000 ---p 00001000 08:02 23069711                   ./libvector.so
7f63ea730000-7f63ea731000 r--p 00000000 08:02 23069711                   ./libvector.so
7f63ea731000-7f63ea732000 rw-p 00001000 08:02 23069711                   ./libvector.so
7f63ea732000-7f63ea755000 r-xp 00000000 08:02 57675621                   /lib/x86_64-linux-gnu/ld-2.19.so
...
```
Finally, the dynamic linker passes control to the application. From this point on, the locations of the shared libraries are fixed and do not change during execution of the program.

#### Loading and Linking Shared Libraries from Applications
Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes. However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time.

Linux systems provide a simple interface to the dynamic linker that allows application programs to load and link shared libraries at run time.

```C
#include <dlfcn.h>

void *dlopen(const char*filename, int flags);
```
The `dlopen` function loads and links the shared library `filename`. The `flag` argument must include either `RTLD_NOW`, which tells the linker to resolve references to external symbols immediately, or the `RTLD_LAZY` flag, which instructs the linker to defer symbol resolution until code from the library is executed.
```C
#include <dlfcn.h>

void *dlsym(void *filename, char *symbol);
```
The `dlsym` function takes a `handle` to a previously opened shared library and a `symbol` name and returns the address of the symbol, if it exists, or NULL otherwise.

```C
#include <stdio.h>
#include <stdlib.h>
#include <dlfcn.h>

int x[2] = {1, 2};
int y[2] = {3, 4};

int z[2];

int main()
{
    void *handle;
    void (*addvec)(int *, int *, int *, int);
    char *error;

    /* Dyamically load the shared library containing addvec() */
    handle = dlopen("./libvector.so", RTLD_LAZY);
    if (!handle) {
        fprintf(stderr, "%s\n", dlerror());
        exit(1);
    }

    /* Get a pointer to the addvec() function we just loaded */
    addvec = dlsym(handle, "addvec");
    if ((error = dlerror()) != NULL) {
        fprintf(stderr, "%s\n", error);
        exit(1);
    }

    /* Now we can call addvec() just like any other function */
    addvec(x, y, z, 2);
    printf("z = [%d %d]\n", z[0], z[1]);

    /* Unload the shared library */
    if (dlclose(handle) < 0) {
        fprintf(stderr, "%s\n", dlerror());
        exit(1);
    }
    return 0;
}
```

Dynamically link our `libvector.so` shared library and then invoke its `addvec` routine. To compile the program, we would invoke GCC in the following way:
```
linux> gcc -rdynamic -o prog2r dll.c -ldl
```

#### Position-Independent Code (PIC)
A key purpose of shared libraries is to allow multiple running processes to share the same library code in memory and thus save precious memory resources. So how can multiple processes share a single copy of a program? One approach would be to assign a priori a dedicated chunk of the address space to each shared library, and then require the loader to always load the shared memory at that address. While straightforward, this approach creates some serious problems. It would be an inefficient use of the address space because portions of the space would be allocated even if a process didn't use the library. It would also be difficult to manage. We would have to ensure that none of the chunks overlapped. Each time a library was modified, we would have to make sure that it still fit in its assigned chunk. If not, then we would have to find a new chunk.

To avoid these problems, modern systems compile the code segments of shared modules so that they can be loaded anywhere in memory without having to be modified by the linker.

Users direct GNU compilation systems to generate PIC code with the `-fpic` option to GCC. Shared libraries must always be compiled with this option.

**PIC Function Calls**

Suppose that a program calls a function that is defined by a shared library. The compiler has no way of predicating the run-time address of the function, since the shared module that defines it could be loaded anywhere at run time. The normal approach would be to generate a relocation record for the reference, which the dynamic linker could then resolve when the program was loaded. However, this approach would not be PIC, since it would require the linker to modify the code segment of the calling module.

The motivation for lazy binding is that a typical application program will call only a handful of the hundreds or thousands of functions exported by a shared library such as `libc.so`. By deferring the resolution of a function's address until it is actually called, the dynamic linker can avoid hundreds or thousands of unnecessary relocations at load time. There is a nontrivial run-time overhead the first time the function is called, but each call thereafter costs only a single instruction and a memory reference for the indirection.

Lazy binding is implemented with a compact yet somewhat complex interaction between two data structures: the GOT and the *procedure linkage table (PLT)*. If an object module calls any functions that are defined in shared libraries, then it has own GOT and PLT. The GOT is part of the data segment. The PLT is part of the code segment.

Figure below shows how the PLT and GOT work together to resolve the address of a function at run time. First, let's examine the contents of each of these tables.
- *Procedure linkage table (PLT).* The PLT is an array of 16-byte code entries. `PLT[0]` is a special entry that jumps into the dynamic linker. Each shared library function called by the executable has own PLT entry. Each of these entries is responsible for invoking a specific function. `PLT[1]` invokes the system startup function (`__libc_start_main`), which initializes the execution environment, calls the `main` function, and handles its return value. Entries starting at `PLT[2]` invoke functions called by the user code. In our example, `PLT[2]` invokes `addvec` and `PLT[3]` invokes `printf`.
- *Global offset table*. As we have seen, the GOT is an array of 8-byte address entries. When used in conjunction with the PLT, `GOT[0]` and `GOT[1]` contain information that the dynamic linker uses when it resolves function addresses. `GOT[2]` is the entry point got the dynamic linker in the `ld-linux.so` module. Each of the remaining entries correspond to a called function whose address needs to be resolved at run time. Each has a matching PLT entry. For example, `GOT[4]` and `PLT[2]` correspond to `addvec`. Initially, each GOT entry points to the second instruction in the corresponding PLT entry.

![Using the PLT and GOT to call external functions.](lazy_binding.png)

Figure (a) shows how the GOT and PLT work together to lazily resolve tun-time address of function `addvec` the first time it is called:

- *Step 1.* Instead of directly calling `addvec`, the program calls into `PLT[2]`, which is the PLT entry for `addvec`.
- *Step 2.* The first PLT instructions does an indirect jump through `GOT[4]`. Since each GOT entry initially points to the second instruction in its corresponding PLT entry, the indirect jump simply transfers control back to the next instruction in `PLT[2]`.
- *Step 3.* After pushing an ID for `addvec` (0x1) onto the stack, `PLT[2]` jumps to `PLT[0]`.
- *Step 4.* `PLT[0]` pushes an argument for the dynamic linker indirectly through `GOT[1]` and then jumps into the dynamic linker indirectly through `GOT[2]`. The dynamic linker uses the two stack entries to determine the run-time location of `addvec`, overwrites `GOT[4]` with this address, and passes control to `addvec`.

Figure (b) shows the control flow for any subsequent invocations of `addvec`.

- *Step 1.* Control passes to `PLT[2]` as before.
- *Step 2.* However, this time the indirect jump through `GOT[4]` transfers control directly `addvec`.

#### Library Interpositioning
Linux linkers support a powerful technique, called *library interpositioning*, that allows you to intercept calls to shared library functions and execute your own code instead. Under interpositioning, you could trace the number of times a particular library function is called, validate and trace its input and output values, or even replace it with a completely different mechanism.

##### Run-Time Interpositioning
Compile-time interpositioning requires access to a program's source files. Link-time interpositioning requires access to its relocatable object files. However, there is a mechanism for interpositioning at run time that requires only to the executable object file. This fascinating mechanism is based on the dynamic linker's `LD_PRELOAD` environment variable.

If the `LD_PRELOAD` environment is set to a list of shared library parameters (separated by spaces or colons), then when you load and execute a program, the dynamic linker (`LD-LINUX.SO`) will search the `LD_PRELOAD` libraries first, before any other shared libraries, when it resolves undefined references. With this mechanism, you can interpose on any function in any shared library, including `libc.so`, when you load and execute any executable.
```C
#ifdef RUNTIME
#define _GNU_SOURCE
#include <stdio.h>
#include <dlfcn.h>

/* malloc wrapper function */
void *malloc(size_t size)
{
    void *(*mallocp)(size_t size);
    char *error;

    mallocp = dlsym(RTLD_NEXT, "malloc"); /* Get address of libc malloc */
    if ((error = dlerror()) != NULL) {
        fputs(error, stderr);
        exit(1);
    }
    char *ptr = mallocp(size); /* Call libc malloc */
    printf("malloc(%d) = %p\n", (int)size, ptr);
    return ptr;
}

/* free wrapper function */
void free(void *ptr)
{
    void (*freep)(void *) = NULL;
    char *error;

    if (!ptr)
        return;

    freep = dlsym(RTLD_NEXT, "free"); /* Get address of libc free */
    if ((error = dlerror()) != NULL) {
        fputs(error, stderr);
        exit(1);
    }
    freep(ptr); /* Call libc free */
    printf("free(%p)\n", ptr);
}
#endif
```
In each wrapper, the call to `dlsym` returns the pointer to the target `libc` function. The wrapper then calls the target function, prints a trace, and returns.

Here is how to build the shared library that contains the wrapper functions:
```
linux> gcc -DRUNTIME -shared -fpic -o mymalloc.so mymalloc.c -ldl
```
Here is how to compile the main program:
```
linux> gcc -o intr int.c
```
Here is how to run the program from the bash shell:
```
linux> LD_PRELOAD="./mymalloc.so" ./intr
malloc(32) = 0x1bf7010
free(0x1bf7010)
```

### Exceptional Control Flow
#### Exceptions
Exceptions are a form of exceptional control flow that are implemented partly by the hardware and partly by the operating system. Because they are partly implemented in hardware, the details vary from system to system. However, the basic ideas are the same for every system. Our aim in this section is to give you a general understanding of exceptions and exception handling and to help demystify what is often a confusing aspect of modern computer systems.

An *exception* is an abrupt change in the control flow in response to some change in the processor's state.

In the figure, the processor is executing some current instruction \\( I_{curr} \\). when a significant change in the processor's *state* occurs. The state is encoded in various bits and signals inside the processor. The change in state is known as an *event*. The event might be directly related to the execution of the current instruction. For example, a virtual memory page fault occurs, an arithmetic overflow occurs, or an instruction attempts a divide by zero. On the other hand, the event might be unrelated to the execution of the current instruction. For example, a system timer goes off or an I/O request completes.

In any case, when the processor detects that the event has occurred, it makes an indirect procedure call (the exception), through a jump table called an *exception table*, to an operating system subroutine (the *exception handler*) that is specifically designed to process this particular kind of event.

##### Exception Handling
Each type of possible exception in a system is assigned a unique nonnegative integer *exception number*. Some of these numbers are assigned by the designers of the processor. Other numbers are assigned by the designers of the operating system *kernel* (the memory-resident part of the operating system). Examples of the former include divide by zero, page faults, memory access violations, breakpoints,and arithmetic overflows. Examples of the latter include system calls and signals from external I/O devices.

At system boot time, the operating system allocates and initializes a jump table called an *exception table*, so that entry \\( k \\) contains the address of the handler for exception \\( k \\).

At run time (when the system is executing some program), the processor detects that an event has occurred and determines the corresponding exception number \\( k \\). The processor then triggers the exception by making an indirect procedure call, through entry \\( k \\) of the exception table, to the corresponding handler. The exception number is an index into the exception table, whose starting address is contained in a special CPU register called the *exception table base register*.

##### Classes of Exceptions
**Interrupts**

*Interrupts* occur *asynchronously* as a result of signals from I/O devices that are external to the processor. Hardware interrupts are asynchronous in the sense that they are not caused by the execution of any particular instruction. Exception handlers for hardware interrupts are often called *interrupt handlers*.

I/O devices such as network adapters, disk controllers, and timer chips trigger interrupts by signaling a pin on the processor chip and placing onto the system bus the exception number that indicates the device that caused the interrupt.

After the current instruction finishes executing, the processor notices that the interrupt pin has gone high, reads the exception number from the system bus, and then calls the appropriate interrupt handler.

**Traps and System Calls**

The most important use of traps is to provide a procedure-like interface between user programs and the kernel, known as *system call*.

User programs often need to request services from the kernel such reading a file (`read`), creating a new process (`fork`), loading a new program (`execve`), and terminating the current process (`exit`).

From a programmer's perspective, a system call is identical to a regular function call. However, their implementations are quite different. Regular functions run in *user mode*, which restricts the types of instructions they can execute, and they access the same stack as the calling function. A system call runs in *kernel mode*, which it allows it to execute privileged instructions and access a stack defined in the kernel.

#### Processes
##### User and Kernel Modes
Processors typically provide this capability with a *mode bit* in some control register that characterizes the privileges the process currently enjoys. When the mode bit is set, the process is running in *kernel mode*. A process running in kernel mode can execute any instruction in the instruction set and access any memory location in the system.

When the mode bit is not set, the process is running in *user mode*. A process in user mode is not allowed to execute *privileged instructions* that do things such as halt the processor, change the mode bit, or initiate an I/O instruction. Nor is it allowed to directly reference code or data in the kernel area of the address space. Any such attempt results in a fatal protection fault.

A process running application code is initially in user mode. The only way for the process to change from user mode to kernel mode is via an exception such as an interrupt, a fault, or a trapping system call. When the exception occurs, and control passes to the exception handler, the processor changes mode from user mode to kernel mode. The handler runs in kernel mode. When it returns to the application mode, the processor changes from kernel mode back to user mode.

##### Context Switches
The operating system kernel implements multitasking using a higher-level form of exceptional control flow known as a *context switch*.

The kernel maintains a *context* for each process. The context is the state that the kernel needs to restart a preempted process. It consists of the values of objects such as the general-purpose registers, the floating-point registers, the program counter, user's stack, status registers, kernel's stack, and various kernel data structures such as a *page table* that characterizes the address space, a *process table* that contains information about the current process, and a *file table* that contains information about the files that the process has opened.

At certain points during the execution of a process, the kernel can decide to preempt the current process and restart a previously preempted process. This decision is known as *scheduling* and is handled by code in the kernel, called the *scheduler*. When the kernel selects a new process to run, we say that the kernel has *scheduled* that process. After the kernel has scheduled a new process to run, it preempts the current process and transfers control to the new process using a mechanism called a *context switch* that (1) saves the context of the current process, (2) restores the saved context of some previously preempted process, and (3) passes control to this newly restored process.

A context switch can occur while the kernel is executing a system call on behalf of the user. If the system call blocks because it is waiting for some event to occur, then the kernel can put the current process to sleep and switch to another process. For example, if a `read` system call requires a disk access, the kernel can opt to perform a context switch and run another process instead of waiting for the data to arrive from the disk.
