# Computer Systems: A Programmer's Perspective
## Program Structure and Execution
### Machine-Level Representation of Programs
#### Combining Control and Data in Machine-Level Programs
##### Thwarting Buffer Overflow Attacks
**Stack Randomization**

In order to insert exploit code into a system, the attacker needs to inject both the code as well as a pointer to this code as part of the attack string. Generating this pointer requires knowing the stack address where the string will be located. Historically, the stack address for a program were highly predictable. For all systems running the same combination of program and operating system version, the stack locations were fairly stable across many machines.

The idea of *stack randomization* is to make the position of the stack vary from one run of a program to another. Thus, even if many machines are running identical code, they would all be using different stack addresses. This is implemented by allocating a random amount of space between 0 and *n* bytes on the stack at the start of a program, for example, by using the allocation function `alloca`, which allocates space for a specified number of bytes on the stack. This allocated space is not used by the program, but it causes all subsequent stack locations to vary from one execution of a program to another. The allocation range *n* needs to be large enough to get sufficient variations in the stack address, yet small enough that it does not waste too much space in the program.

The following code shows a simple way to determine a "typical" stack address:
```
int main() {
    long local;
    printf("local at %p\n", &local);
    return 0;
}
```
This code simply prints the address of a local variable in the `main` function. Running the code 10000 times on a Linux machine in 32-bit mode, the address ranged from `0xff7fc59c` to `0xffffd09c`, a range of around 2<sup>23</sup>.

Stack randomization has become standard practice in Linux systems. It is one of a larger class technique known as *address-space layout randomization*, or ASLR. With ASLR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different regions of memory each time a program is run. That means that a program running on one machine will have very different address mappings than the same program running on other machines. This can thwart some forms of attack.

### The Memory Hierarchy
#### Storage Technologies
##### Random Access Memory
**Nonvolatile Memory**

DRAMs and SRAMs are *volatile* in the sense that they lose their information if the supply voltage is turned off. *Nonvolatile memories*, on the other hand, retain their information even when they are powered off. There are a variety of nonvolatile memories. For historical reasons, they are referred to collectively as *read-only memories* (ROMs), even though some types of ROMs can be written to as well as read. ROMs are distinguished by the number of times they can be reprogrammed (written to) and by the mechanism for reprogramming them.

A *programmable ROM (PROM)* can be programmed exactly once. PROMs include a sort of fuse with each memory cell that can be blown once by zapping it with a high current.

An *erasable programmable ROM (EPROM)* has a transparent quartz window that permits light to reach the storage cells. The EPROM cells are cleared to zeros by shinning ultraviolet light though the window. Programming an EPROM is done by using a special device to write ones into the EPROM. An EPROM can be erased and reprogrammed on the order of 1000 times. An *electrically erasable PROM (EEPROM)* is akin to an EPROM, but it does not require a physically separate programming device, and thus can be reprogrammed in-place on printed circuit cards. An EEPROM can be reprogrammed on the order of 100000 times before it wears out.

*Flash memory* is a type of nonvolatile memory, based on EEPROMs, that has become an important storage technology. We will look in detail at a new form of flash-based disk drive, known as a *solid state disk (SSD)*, that provides a faster, sturdier, and less power-hungry alternative to conventional rotating disks.

##### Disk Storage
*Disks* are workhorse storage devices that hold enormous amounts of data, on the order of hundreds to thousands of gigabytes, as opposed to the hundreds or thousands of megabytes in a RAM-based memory.

##### Solid State Disks
A solid state disk (SSD) is a storage technology, based on flash memory, that in some situations is an attractive alternative to the conventional rotating disk.

![Solid state disk (SSD).](ssd.png)

An SSD package plugs into a standard disk slot on the I/O bus (typically USB or SATA) and behaves like any other disk, processing requests from the CPU to read and write logical disk blocks. An SSD package consists of one or more flash memory chips, which replace the mechanical driver in a conventional rotating disk, and a *flash translation layer*, which is a hardware/firmware device that plays the same role as a disk controller, translating requests for logical blocks into accesses of the underlying physical device.

A flash memory consists of a sequence of *B blocks*, where each block consists of *P* pages. Typically, pages are 512 bytes to 4KB in size, and a block consists of 32-128 pages, with total block sizes ranging from 16KB to 512KB. Data are read and written in units of pages. A page can be written only after the entire block to which it belongs has been *erased* (typically, this means that all bits in the block are set to 1). However, once a block is erased, each page in the block can be written once with no further erasing. A block wears out after roughly 100000 repeated writes. Once a block wears out, it can no longer be erasing.

Random writes are slower for two reasons. First, erasing a block takes a relatively long time, on the order of 1ms, which is more than an order to magnitude longer than it takes to access a page. Second, if a write operation attempts to modify a page *p* that contains existing data (i.e., not all ones), then any pages in the same block with useful data must be copied to a new (erased) block before the write to page *p* can occur.

SSDs have a number of advantages over rotating disks. They are built of semiconductor memory, with no moving parts, and thus have much faster random access times than rotating disks, use less power, and are more rugged.

#### Cache Memories
The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main memory, and disk storage. However, because of the increasing gap between CPU and main memory, system designers were compelled to insert a small SRAM *cache memory*, called an *L1 cache* (level 1 cache) between the CPU register file and main memory. The L1 cache can be accessed nearly as fast as the registers, typically in about 4 clock cycles.

![Typical bus structure for cache memories.](cache_memories.png)

As the performance gap between the CPU and main memory continued to increase, system designers responded by inserting an additional larger cache, called an *L2 cache*, between the L1 cache and main memory, that can be accessed in about 10 clock cycles. Many modern systems include an even larger cache, called an *L3 cache*, which sits between the L2 cache and main memory in the memory hierarchy and can be accessed in about 50 cycles.

##### Generic Cache Memory Organization
A cache for such a machine is organized as an array of *cache sets*. Each set consists of *E* *cache lines*. Each cache line consists of a data block of *B* = 2<sup>*b*</sup> bytes, a *valid bit* that indicates whether or not the line contains meaningful information, and *t* = *m* - (*b* + *s*) tag bits (a subset of the bits from the current block's memory address) that uniquely identify the block stored in the cache line.

In general, a cache's organization can be characterized by the tuple (*S*, *E*, *B*, *m*). The size (or capacity) of a cache, *C*, is stated in terms of the aggregate size of all the blocks. Thus, *C* = *S* x *E* x *B*.

When the CPU is instructed by a load instructor to read a word from address *A* of main memory, it sends address *A* to the cache. If the cache is holding a copy of the word at address *A*, it sends the word immediately back to the CPU. So how does the cache know whether it contains a copy of the word at address *A*? The cache is organized so that it can find the requested word by simply inspecting the bits of the address, similar to a hash table with an extremely simple hash function.

##### Issues with Writes
As we have seen, the operation of a cache with respect to reads is straightforward. First, look for a copy of the desired word *w* in the cache. If there is a hit, return *w* immediately. If there is a miss, fetch the block that contains *w* from the next lower level of the memory hierarchy, store the block in some cache line (possibly evicting a valid line), and then return *w*.

The situation for writes is a little more complicated. Suppose we write a word *w* that is already cached (a *write hit*). After the cache updates its copy of *w*, what does it do about updating the copy of *w* in th next lower level of the hierarchy? The simplest approach, known as *write-through*, is to immediately write *w*'s cache block to the next lower level. While simple, write-through has the disadvantage of causing bus traffic with every write. Another approach, known as *write-back*, defers the update as long as possible by writing the updated block to the next lower level when it is evicted from the cache by the replacement algorithm. Because of locality, write-back can significantly reduce the amount of bus traffic, but it has the advantage pf additional complexity. The cache must maintain an additional *dirty bit* for each cache line that indicates whether or not the cache block has been modified.

## Running Programs on a System
### Linking
#### Compiler Drivers
```
(a)main.c                                              (b)sum.c
-------------------------------- code/link/main.c      -------------------------------- code/link/sum.c

1    int sum(int *a, int n)                            1    int sum(int *a, int n)
2                                                      2    {
3    int array[2] = {1, 2};                            3        int i, s = 0;  
4                                                      4
5    int main()                                        5        for (i = 0; i < n; i++) {
6    {                                                 6            s += a[i];
7        int val = sum(array, 2);                      7        }
8        return val;                                   8        return s;
9    }                                                 9    }

-------------------------- code/link/main.c            -------------------------------- code/link/sum.c
```
Most compilation systems provide a *compile driver* that invokes the language preprocessor, compiler, assembler, and linker, as needed on the behalf of the user. For example, to build the example program using the GNU compilation system, we might invoke the GCC driver by typing the following command to the shell:
```
linux> gcc -Og -o prog main.c sum.c
```
The driver first runs the C preprocessor (`cpp`), which translates the C source file `main.c` into an ASCII intermediate file `main.i`:
```
cpp [other arguments] main.c /tmp/main.i
```
Next, the driver runs the C compiler (`cc1`), which translates `main.i` into an ASCII assembly-language file `main.s`:
```
cc1 /tmp/main.i -Og [other arguments] -o /tmp/main.s
```
Then, the driver runs the assembler (`as`), which translates `main.s` into a binary *relocatable object file* `main.o`:
```
as [other arguments] -o /tmp/main.o /tmp/main.s
```
The driver goes through the same process to generate `sum.o`. Finally, it runs the linker program `ld`, which combines `main.o` and `sum.o`, along with the necessary system object files, to create the binary *executable object file* `prog`:
```
ld -o prog [system object files and args] /tmp/main.o /tmp/sum.o
```
To run the executable `prog`, we type its name on the Linux shell's command line:
```
linux> ./prog
```
The shell invokes a function in the operating system called the *loader*, which copies the code and data in the executable file `prog` into memory, and then transfers control to the beginning of the program.

#### Relocatable Object Files
.text: The machine code of the compiled program.

.data: *Initialized* global and static C variables. Local C variables are maintained at run time on the stack, and do *not* appear in either the .data or .bss sections.

.bss: *Uninitialized* global and static C variables, along with any global or static variables that are initialized to zero.

.symtab: A *symbol table* with information about functions and global variables that are defined and referenced in the program.

#### Symbols and Symbol Tables
Each relocatable object module, *m*, has a symbol table that contains information about the symbols defined and referenced by *m*. In the context of a linker, there are three different kinds of symbols:
- *Global symbols* that are defined by module *m* and that can be referenced by other modules.
- Global symbols that are referenced by module *m* but defined by some other module.
Such symbols are called *externals* and correspond to nonstatic C functions and global variables that are defined in other modules.
- *Local symbols* that are defined and referenced exclusively by module *m*.

Symbol tables are built by assemblers, using symbols exported by the compiler into the assembly language. An ELF symbol table is contained in the `.symtab` section. It contains an array of entries.

The GNU `READELF` program is a handy tool for viewing the contents of object files: For example, here are the last three symbol table entries for the relocatable object file `main.o`. The first eight entries, which are not shown, are local symbols that the linker uses internally.
```
Num:    Value          Size Type    Bind   Vis      Ndx Name
  8: 0000000000000000    24 FUNC    GLOBAL DEFAULT    1 main
  9: 0000000000000000     8 OBJECT  GLOBAL DEFAULT    3 array
 10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND sum
```
In this example, we see an entry for the definition of global symbol `main`, a 24-byte function located at an offset (i.e., `value`) of zero in the `.text` section. This is followed by the definition of the global symbol `array`, an 8-byte object located at an offset of zero in the `.data` section. The last entry comes from the reference to the external symbol `sum`. `READELF` identifies each section by an integer index. `Ndx=1` denotes the `.text` section, and `Ndx=3` denotes the `.data` section.

#### Symbol Resolution
Resolving references to global symbols, however, is trickier. When the compiler encounters a symbol (either a variable or function name) that is not defined in the current module, it assumes that it is defined in some other module, generates a linker symbol table entry, and leaves it for the linker to handle. If the linker is unable to find a definition for the referenced symbol in any of input modules, it prints an error message and terminates. For example, if we try to compile and link the following source file on a Linux machine,
```
void foo(void);

int main() {
	foo();
	return 0;
}
```
then the compiler runs without a hitch, but the linker terminates when it cannot resolve the reference to `foo`:
```
linux> gcc -Wall -Og -o linkerror linkerror.c 
/tmp/ccSz5uti.o: In function `main':
/tmp/ccSz5uti.o(.text+0x7): undefined reference to `foo'
```
Symbol resolution for global symbols is also tricky because the same symbol might be defined by multiple object files. In this case, the linker must either flag an error or somehow choose one of the definitions and discard the rest.

##### How Linkers Resolve Duplicate Global Symbols
At compile time, the compiler exports each global symbol to the assembler as either *strong* or *weak*, and the assembler encodes this information implicitly in the symbol table of the relocatable object file. Functions and initialized global variables get strong symbols. Uninitialized global variables get weak symbols.

Given this notion of strong and weak symbols, Linux linkers use the following rules for dealing with multiply duplicate symbol names:
- Rule 1: Multiple strong symbols with the same name are not allowed.

For example, suppose we attempt to compile and link the following two C modules:
```
1    /* foo1.c */
2    int main()
3    {
4        return 0;
5    }

1    /* bar1.c */
2    int main()
3    {
4        return 0;
5    }
```
In this case, the linker will generate an error message because the strong symbol `main` is defined in multiple times (rule 1):
```
linux> gcc foo1.c bar1.c
/tmp/ccq2Uxnd.o: In function `main':
/tmp/ccq2Uxnd.o(.text+0x0): multiple definition of `main'
```

##### Linking with Static Libraries
```
(a)addvec.o                                           (b)multvec.c
----------------------------- code/link/addvec.c      ----------------------------- code/link/multvec.c

 1    int addcnt = 0;                                  1    int multcnt = 0;
 2                                                     2
 3    void addvec(int *x, int *y,                      3    void multvec(int *x, int *y,
 4                int *z, int n)                       4                 int *z, int n)
 5    {                                                5    {
 6        int i;                                       6        int i;
 7                                                     7
 8        addcnt++;                                    8        multcnt++;
 9                                                     9
10        for (i = 0; i < n; i++)                     10        for (i = 0; i < n; i++)
11            z[i] = x[i] + y[i];                     11            z[i] = x[i] * y[i];
12    }                                               12    }
----------------------------- code/link/addvec.c      ----------------------------- code/link/multvec.c
```

To create a static library of these functions, we would use the `AR` tool as follows:
```
linux> gcc -c addvec.c multvec.c
linux> ar rcs libvector.a addvec.o multvec.o
```
To build the executable, we would compile and link the input file `main2.o` and `libvector.a`:
```
linux> gcc -c main2.c
linux> gcc -static -o prog2c main2.o ./libvector.a
```
Or equivalently,
```
linux> gcc -c main2.c
linux> gcc -static -o prog2c main2.o -L. -lvector
```
The `-static` argument tells the compiler driver that the linker should build a fully linked executable object file that can be loaded into memory and run without any further linking at load time. The `-lvector` argument is a shorthand for `libvector.a`, and the `-L.` argument tells the linker to look for `libvector.a` in the current directory.

When the linker runs, it determines that the `addvec` symbol defined by `addvec.o` is referenced by `main.o`, so it copies `addvec.o` into the executable. Since the program doesn't reference any symbols defined by `multvec.o`, the linker does not copy this module into the executable. The linker also copies `printf.o` module from `libc.a`, along with a number of other modules from the C run-time system.

##### How Linkers Use Static Libraries to Resolve References
During the symbol resolution phase, the linker scans the relocatable object files and archives left to right in the same sequential order that they appear on the compiler driver's command line. During this scan, the linker maintains a set *E* of relocatable object files that will be merged to form the executable, a set *U* of unresolved symbols, and a set *D* of symbols that have been defined in previous input files.
- For each input file *f* on the command line, the linker determines if *f* is an object file or an archive. If *f* is an object file, the linker adds *f* to *E*, updates *U* and *D* to reflect the symbol definitions and references in *f*, and proceeds to the next input file.
- If *f* is an archive, the linker attempts to match the unresolved symbols in *U* against the symbols defined by the members of the archive. If some archive member, *m*, defines a symbol that resolves a reference in *U*, then *m* is added to *E*, and the linker updates *U* and *D* to reflect the symbol definitions and references in *m*. This process iterates over the member object files in the archive until a fixed point is reached where *U* and *D* no longer change. At this point, any member object files not contained in *E* are simply discarded and the linker proceeds to the next input file.
- If *U* is nonempty when the linker finishes scanning the input files on the command line, it prints an error and terminates. Otherwise, it merges and relocates the object files in *E* to build the output executable file.

If the library that defines a symbol appears on the command line before the object file that references that symbol, then the reference will not be resolved and linking will fail. For example, consider the following:
```
linux> gcc -static ./libvector.a main2.c
```
When `libvector.a` is processed, *U* is empty, so no member object files from `libvector.a` are added to *E*. Thus, the reference to `addvec` is never resolved and the linker emits an error message and terminates.

GNU linker option: `--as-needed` and `--no-as-needed`

This option affects ELF `DT_NEEDED` tags for **dynamic libraries** mentioned on the command line after the `--as-needed` option. Normally the linker will add a `DT_NEEDED` tag for each dynamic library mentioned on the command line, regardless of whether the library is actually needed or not. `--as-needed` causes a `DT_NEEDED` tag to only be emitted for a library that at that point in the link satisfies a non-weak undefined symbol reference from a regular object file or, if the library is not found in the `DT_NEEDED` lists of other libraries, a non-weak undefined symbol reference from another dynamic library. Object files or libraries appearing on the command line after the library in question do not affect whether the library is seen as needed. `--no-as-needed` restores the default behaviour.

#### Relocation
Once the linker has completed the symbol resolution step, it has associated each symbol reference in the code with exactly one symbol definition (i.e., a symbol table entry in one of its input object modules). At this point, the linker knows the exact sizes of the code and data sections in its input object modules. It is now ready to begin the relocation step, where it merges the input modules and assigns run-time addresses to each symbol. Relocation consists of two steps:
1. *Relocating sections and symbol definitions.* In this step, the linker merges all sections of the same type into a new aggregate section of the same type. For example, the `.data` sections from the input modules are all merged into one section that will become the `.data` section for the output executable object file. The linker then assigns run-time memory addresses to the new aggregate sections, to each section defined by the input modules, and to each symbol defined by the input modules. When this step is complete, each instruction and global variable in the program has a unique run-time memory address.
2. *Relocating symbol references within sections.* In this step, the linker modifies every symbol reference in the bodies of the code and data sections so that they point to the correct tun-time address.

##### Relocation Entries
When an assembler generates an object module, it does not know where the code and data will ultimately be stored in memory. Nor does it know the locations of any externally defined functions or global variables that are referenced by the module. So whenever the assembler encounters a reference to an object whose ultimate location is unknown, it generates a *relocation entry* that tells the linker how to modify the reference when it merges the object file into an executable.

#### Executable Object Files
We have seen how the linker merges multiple object files into a single executable object file.

The format of an executable object file is similar to that of a relocatable object file. The ELF header describes the overall format of the file. It also includes the program's *entry point*, which is the address of the first instruction to execute when the program runs. The `.text`, `.rodata`, and `.data` sections are similar to those in a relocatable object file, except that these sections have been relocated to their eventual run-time memory addresses.

#### Loading Executable Object Files

To run an executable object file `prog`, we can type its name to the Unix shell's command line:
```
linux> ./prog
```
Since `prog` does not correspond to a built-in shell command, the shell assumes that `prog` is an executable object file, which it runs for us by invoking some memory-resident operating system code known as the loader. The loader copies the code and data in the executable object file from disk into memory, and then runs the program by jumping to its first instruction, or *entry point*. This process of copying the program into memory and then running it is known as *loading*.

![Linux x86-64 run-time memory image.](run-time_memory.png)

#### Dynamic Linking with Shared Libraries
Another issue is that almost every C program uses standard I/O functions such as `printf` and `scanf`. At run time, the code for these functions is duplicated in the text segment of each running process. On a typical system that is running hundreds of processes, this can be a significant waste of scarce memory system resources.

*Shared libraries* are modern innovations that address the disadvantages of static libraries. A shared library is an object module that, at run time, can be loaded at an arbitrary memory address and linked with a program in memory. This process is known as *dynamic linking* and is performed by a program called a *dynamic linker*.

Shared libraries are "shared" in two different ways. First, in any given file system, there is exactly only one `.so` file for a particular library. The code and data in this `.so` file are shared by all of the executable object files that reference the library, as opposed to the contents of static libraries, which are copied and embedded in the executables that reference them. Second, a single copy of the `.text` section of a shared library in memory can be shared by different running processes.

To build a shared library `libvector.so` of our example vector routines, we would invoke the compiler driver with the following special directive to the compiler and linker:
```
linux> gcc -shared -fpic -o libvector.so addvec.c multvec.c
```
The `-shared` flag directs the linker to create a shared object file. Once we have created the library, we would then link it into our example program:
```
linux> gcc -o prog21 main2.c ./libvector.so
```
This creates an executable object file `prog21` in a form that can be linked with `libvector.so` at run time. The basic idea is to do some of the linking statistically when the executable file is created, and then complete the linking process dynamically when the program is loaded. It is important to realize that none of the code or data sections from `libvector.so` are actually copied into the executable `prog21` at this point.

When the loader loads and runs the executable `prog21`, it loads the partially linked executable `prog21`. Next it notices that `prog21` contains a `.interp` section, which contains the name of the dynamic linker, which is itself a shared object (e.g., `ld-linux.so` on Linux systems). Instead of passing control to the application, as it would normally do, the loader loads an runs the dynamic linker. The dynamic linker then finishes the linking task by performing the following relocations:
- Relocating the text and data of `libc.so` into some memory segment.
- Relocating the text and data of `libvector.so` into another memory segment.
- Relocating any references in `prog21` to symbols defined by `libc.so` and `libvector.so`.

```
$ cat /proc/3870/maps
# before addvec() is invoked
...
7f63ea16a000-7f63ea326000 r-xp 00000000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea326000-7f63ea525000 ---p 001bc000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea525000-7f63ea529000 r--p 001bb000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea529000-7f63ea52b000 rw-p 001bf000 08:02 57675645                   /lib/x86_64-linux-gnu/libc-2.19.so
7f63ea52b000-7f63ea530000 rw-p 00000000 00:00 0
7f63ea530000-7f63ea531000 r-xp 00000000 08:02 23069711                   ./libvector.so
7f63ea531000-7f63ea730000 ---p 00001000 08:02 23069711                   ./libvector.so
7f63ea730000-7f63ea731000 r--p 00000000 08:02 23069711                   ./libvector.so
7f63ea731000-7f63ea732000 rw-p 00001000 08:02 23069711                   ./libvector.so
7f63ea732000-7f63ea755000 r-xp 00000000 08:02 57675621                   /lib/x86_64-linux-gnu/ld-2.19.so
...
```
Finally, the dynamic linker passes control to the application. From this point on, the locations of the shared libraries are fixed and do not change during execution of the program.

#### Loading and Linking Shared Libraries from Applications
Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes. However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time.

Linux systems provide a simple interface to the dynamic linker that allows application programs to load and link shared libraries at run time.

```C
#include <dlfcn.h>

void *dlopen(const char*filename, int flags);
```
The `dlopen` function loads and links the shared library `filename`. The `flag` argument must include either `RTLD_NOW`, which tells the linker to resolve references to external symbols immediately, or the `RTLD_LAZY` flag, which instructs the linker to defer symbol resolution until code from the library is executed.
```C
#include <dlfcn.h>

void *dlsym(void *filename, char *symbol);
```
The `dlsym` function takes a `handle` to a previously opened shared library and a `symbol` name and returns the address of the symbol, if it exists, or NULL otherwise.

```C
#include <stdio.h>
#include <stdlib.h>
#include <dlfcn.h>

int x[2] = {1, 2};
int y[2] = {3, 4};

int z[2];

int main()
{
    void *handle;
    void (*addvec)(int *, int *, int *, int);
    char *error;

    /* Dyamically load the shared library containing addvec() */
    handle = dlopen("./libvector.so", RTLD_LAZY);
    if (!handle) {
        fprintf(stderr, "%s\n", dlerror());
        exit(1);
    }

    /* Get a pointer to the addvec() function we just loaded */
    addvec = dlsym(handle, "addvec");
    if ((error = dlerror()) != NULL) {
        fprintf(stderr, "%s\n", error);
        exit(1);
    }

    /* Now we can call addvec() just like any other function */
    addvec(x, y, z, 2);
    printf("z = [%d %d]\n", z[0], z[1]);

    /* Unload the shared library */
    if (dlclose(handle) < 0) {
        fprintf(stderr, "%s\n", dlerror());
        exit(1);
    }
    return 0;
}
```

Dynamically link our `libvector.so` shared library and then invoke its `addvec` routine. To compile the program, we would invoke GCC in the following way:
```
linux> gcc -rdynamic -o prog2r dll.c -ldl
```

#### Position-Independent Code (PIC)
A key purpose of shared libraries is to allow multiple running processes to share the same library code in memory and thus save precious memory resources. So how can multiple processes share a single copy of a program? One approach would be to assign a priori a dedicated chunk of the address space to each shared library, and then require the loader to always load the shared memory at that address. While straightforward, this approach creates some serious problems. It would be an inefficient use of the address space because portions of the space would be allocated even if a process didn't use the library. It would also be difficult to manage. We would have to ensure that none of the chunks overlapped. Each time a library was modified, we would have to make sure that it still fit in its assigned chunk. If not, then we would have to find a new chunk.

To avoid these problems, modern systems compile the code segments of shared modules so that they can be loaded anywhere in memory without having to be modified by the linker.

Users direct GNU compilation systems to generate PIC code with the `-fpic` option to GCC. Shared libraries must always be compiled with this option.

**PIC Function Calls**

Suppose that a program calls a function that is defined by a shared library. The compiler has no way of predicating the run-time address of the function, since the shared module that defines it could be loaded anywhere at run time. The normal approach would be to generate a relocation record for the reference, which the dynamic linker could then resolve when the program was loaded. However, this approach would not be PIC, since it would require the linker to modify the code segment of the calling module.

The motivation for lazy binding is that a typical application program will call only a handful of the hundreds or thousands of functions exported by a shared library such as `libc.so`. By deferring the resolution of a function's address until it is actually called, the dynamic linker can avoid hundreds or thousands of unnecessary relocations at load time. There is a nontrivial run-time overhead the first time the function is called, but each call thereafter costs only a single instruction and a memory reference for the indirection.

Lazy binding is implemented with a compact yet somewhat complex interaction between two data structures: the GOT and the *procedure linkage table (PLT)*. If an object module calls any functions that are defined in shared libraries, then it has own GOT and PLT. The GOT is part of the data segment. The PLT is part of the code segment.

Figure below shows how the PLT and GOT work together to resolve the address of a function at run time. First, let's examine the contents of each of these tables.
* *Procedure linkage table (PLT).* The PLT is an array of 16-byte code entries. `PLT[0]` is a special entry that jumps into the dynamic linker. Each shared library function called by the executable has own PLT entry. Each of these entries is responsible for invoking a specific function. `PLT[1]` invokes the system startup function (`__libc_start_main`), which initializes the execution environment, calls the `main` function, and handles its return value. Entries starting at `PLT[2]` invoke functions called by the user code. In our example, `PLT[2]` invokes `addvec` and `PLT[3]` invokes `printf`.
* *Global offset table*. As we have seen, the GOT is an array of 8-byte address entries. When used in conjunction with the PLT, `GOT[0]` and `GOT[1]` contain information that the dynamic linker uses when it resolves function addresses. `GOT[2]` is the entry point got the dynamic linker in the `ld-linux.so` module. Each of the remaining entries correspond to a called function whose address needs to be resolved at run time. Each has a matching PLT entry. For example, `GOT[4]` and `PLT[2]` correspond to `addvec`. Initially, each GOT entry points to the second instruction in the corresponding PLT entry.

![Using the PLT and GOT to call external functions.](lazy_binding.png)

Figure (a) shows how the GOT and PLT work together to lazily resolve tun-time address of function `addvec` the first time it is called:
* *Step 1.* Instead of directly calling `addvec`, the program calls into `PLT[2]`, which is the PLT entry for `addvec`.
* *Step 2.* The first PLT instructions does an indirect jump through `GOT[4]`. Since each GOT entry initially points to the second instruction in its corresponding PLT entry, the indirect jump simply transfers control back to the next instruction in `PLT[2]`.
* *Step 3.* After pushing an ID for `addvec` (0x1) onto the stack, `PLT[2]` jumps to `PLT[0]`.
* *Step 4.* `PLT[0]` pushes an argument for the dynamic linker indirectly through `GOT[1]` and then jumps into the dynamic linker indirectly through `GOT[2]`. The dynamic linker uses the two stack entries to determine the run-time location of `addvec`, overwrites `GOT[4]` with this address, and passes control to `addvec`.

Figure (b) shows the control flow for any subsequent invocations of `addvec`.
* *Step 1.* Control passes to `PLT[2]` as before.
* *Step 2.* However, this time the indirect jump through `GOT[4]` transfers control directly `addvec`.

#### Library Interpositioning
Linux linkers support a powerful technique, called *library interpositioning*, that allows you to intercept calls to shared library functions and execute your own code instead. Under interpositioning, you could trace the number of times a particular library function is called, validate and trace its input and output values, or even replace it with a completely different mechanism.

##### Run-Time Interpositioning
Compile-time interpositioning requires access to a program's source files. Link-time interpositioning requires access to its relocatable object files. However, there is a mechanism for interpositioning at run time that requires only to the executable object file. This fascinating mechanism is based on the dynamic linker's `LD_PRELOAD` environment variable.

If the `LD_PRELOAD` environment is set to a list of shared library parameters (separated by spaces or colons), then when you load and execute a program, the dynamic linker (`LD-LINUX.SO`) will search the `LD_PRELOAD` libraries first, before any other shared libraries, when it resolves undefined references. With this mechanism, you can interpose on any function in any shared library, including `libc.so`, when you load and execute any executable.
```C
#ifdef RUNTIME
#define _GNU_SOURCE
#include <stdio.h>
#include <dlfcn.h>

/* malloc wrapper function */
void *malloc(size_t size)
{
    void *(*mallocp)(size_t size);
    char *error;

    mallocp = dlsym(RTLD_NEXT, "malloc"); /* Get address of libc malloc */
    if ((error = dlerror()) != NULL) {
        fputs(error, stderr);
        exit(1);
    }
    char *ptr = mallocp(size); /* Call libc malloc */
    printf("malloc(%d) = %p\n", (int)size, ptr);
    return ptr;
}

/* free wrapper function */
void free(void *ptr)
{
    void (*freep)(void *) = NULL;
    char *error;

    if (!ptr)
        return;

    freep = dlsym(RTLD_NEXT, "free"); /* Get address of libc free */
    if ((error = dlerror()) != NULL) {
        fputs(error, stderr);
        exit(1);
    }
    freep(ptr); /* Call libc free */
    printf("free(%p)\n", ptr);
}
#endif
```
In each wrapper, the call to `dlsym` returns the pointer to the target `libc` function. The wrapper then calls the target function, prints a trace, and returns.

Here is how to build the shared library that contains the wrapper functions:
```
linux> gcc -DRUNTIME -shared -fpic -o mymalloc.so mymalloc.c -ldl
```
Here is how to compile the main program:
```
linux> gcc -o intr int.c
```
Here is how to run the program from the bash shell:
```
linux> LD_PRELOAD="./mymalloc.so" ./intr
malloc(32) = 0x1bf7010
free(0x1bf7010)
```
